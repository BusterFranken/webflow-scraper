<!DOCTYPE html><!-- Last Published: Fri Apr 18 2025 09:22:50 GMT+0000 (Coordinated Universal Time) --><html data-wf-domain="www.fruitpunch.ai" data-wf-page="661d09a4061bc8f0cc423c89" data-wf-site="623d7d246e8213dbe6aace84" data-wf-collection="661d09a4061bc8f0cc423c7d" data-wf-item-slug="the-pains-of-classifying-flooded-forests-in-satellite-data"><head><meta charset="utf-8"/><title>The pains of classifying flooded forests in satellite data</title><meta content="A tricky detection use case - from weeks of data pre-processing to training 2 CNNs; and why the answer might be in infrared band data.Sep 21, 2022" name="description"/><meta content="The pains of classifying flooded forests in satellite data" property="og:title"/><meta content="A tricky detection use case - from weeks of data pre-processing to training 2 CNNs; and why the answer might be in infrared band data.Sep 21, 2022" property="og:description"/><meta content="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6641d523ea67321016aa9c82_header.jpeg" property="og:image"/><meta content="The pains of classifying flooded forests in satellite data" property="twitter:title"/><meta content="A tricky detection use case - from weeks of data pre-processing to training 2 CNNs; and why the answer might be in infrared band data.Sep 21, 2022" property="twitter:description"/><meta content="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6641d523ea67321016aa9c82_header.jpeg" property="twitter:image"/><meta property="og:type" content="website"/><meta content="summary_large_image" name="twitter:card"/><meta content="width=device-width, initial-scale=1" name="viewport"/><meta content="3W8ATyTMYMESrts8sQd9lyAvrN8Svo_I9Ks_ZE2td3I" name="google-site-verification"/><link href="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/css/fruitpunch-ai.webflow.e858c30b5.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com" rel="preconnect"/><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous"/><script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script><script type="text/javascript">WebFont.load({  google: {    families: ["Atkinson Hyperlegible:regular,700","Roboto Mono:regular,600,700","Inter:100,200,300,regular,500,600,700,800,900"]  }});</script><script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script><link href="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/62460f7d0eb74f5a342f656e_32x32.svg" rel="shortcut icon" type="image/x-icon"/><link href="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/62460f835d0f30b3e1e8e5fb_256x256.svg" rel="apple-touch-icon"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-EXZXZ384T7"></script><script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('set', 'developer_id.dZGVlNj', true);gtag('js', new Date());gtag('config', 'G-EXZXZ384T7');</script><script type="text/javascript">
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "jby5w04mfa");
</script>



<!--Tooltip Styling-->
<link rel="stylesheet" href="https://unpkg.com/tippy.js@4/themes/light-border.css"/>
<style>
.blog-post_image-height {
	-webkit-backface-visibility: hidden;
	-moz-backface-visibility: hidden;
	-webkit-transform: translate3d(0, 0, 0);
	-moz-transform: translate3d(0, 0, 0)
}
</style>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-H92D1LL95W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EXZXZ384T7');
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-PMKPDFGL');</script>
<!-- End Google Tag Manager -->


<script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/274b78179f279b8ff0a6f8164/1de32b2c861555e2768ba71e2.js");</script>

<script>!function () {var reb2b = window.reb2b = window.reb2b || [];if (reb2b.invoked) return;reb2b.invoked = true;reb2b.methods = ["identify", "collect"];reb2b.factory = function (method) {return function () {var args = Array.prototype.slice.call(arguments);args.unshift(method);reb2b.push(args);return reb2b;};};for (var i = 0; i < reb2b.methods.length; i++) {var key = reb2b.methods[i];reb2b[key] = reb2b.factory(key);}reb2b.load = function (key) {var script = document.createElement("script");script.type = "text/javascript";script.async = true;script.src = "https://s3-us-west-2.amazonaws.com/b2bjsstore/b/" + key + "/4O7Z0HM2DWNX.js.gz";var first = document.getElementsByTagName("script")[0];first.parentNode.insertBefore(script, first);};reb2b.SNIPPET_VERSION = "1.0.1";reb2b.load("4O7Z0HM2DWNX");}();</script><script>
const canonicalUrl = "https://app.fruitpunch.ai/article/2022/09/21/the-pains-of-classifying-flooded-forests-in-satellite-data";
if (canonicalUrl != "") {
	var canonicalEl = document.createElement("link");
	canonicalEl.rel = "canonical";
	canonicalEl.href = canonicalUrl;
	document.head.appendChild(canonicalEl);
}
</script>

<!-- [Attributes by Finsweet] CMS PrevNext -->
<script async src="https://cdn.jsdelivr.net/npm/@finsweet/attributes-cmsprevnext@1/cmsprevnext.js"></script>

<!-- [Attributes by Finsweet] CMS Nest -->
<script async src="https://cdn.jsdelivr.net/npm/@finsweet/attributes-cmsnest@1/cmsnest.js"></script>
</head><body><div data-w-id="d162ae7a-f1fa-352e-4672-062769f40e40" data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="navbar-2 w-nav"><div class="a-banner-announcement"><div class="a-banner-container-small"><div class="a-banner-text-wrapper"><div class="a-banner-icon"></div></div><div class="a-banner-right-wrapper"><div data-hover="false" data-delay="0" class="w-dropdown"><div class="a-banner-toggle-copy w-dropdown-toggle"><div class="a-caption-copy">For Individuals</div></div><nav class="a-banner-list w-dropdown-list"></nav></div><div data-hover="false" data-delay="0" class="w-dropdown"><div class="a-banner-toggle w-dropdown-toggle"><a href="/partners" class="w-inline-block"><div class="a-caption">For Organizations</div></a></div><nav class="a-banner-list w-dropdown-list"></nav></div></div></div></div><div class="uui-navbar02_component-3"><div class="uui-navbar02_container-3"><a href="/" class="uui-navbar02_logo-link-3 w-nav-brand"><div class="uui-logo_component-4"><img src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/624447bdff22b73c4a02373f_Logo-horizontal-colour.svg" loading="lazy" alt="FruitPunch AI logo" class="uui-logo_logotype-3"/><img src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/652797dd2601a8abb837b8f7_untitled-ui-logo.png" loading="lazy" alt="Logo" class="uui-logo_image-3"/></div></a><nav role="navigation" class="uui-navbar02_menu-3 w-nav-menu"><div class="uui-navbar02_menu-left-3"><a href="https://app.fruitpunch.ai/challenges" class="uui-navbar02_link-3 w-nav-link">Challenges</a><div data-hover="true" data-delay="300" data-w-id="d162ae7a-f1fa-352e-4672-062769f40e58" class="uui-navbar02_menu-dropdown-3 w-dropdown"><div class="uui-navbar02_dropdown-toggle-3 w-dropdown-toggle"><div class="uui-dropdown-icon-3 w-embed"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M5 7.5L10 12.5L15 7.5" stroke="currentColor" stroke-width="1.67" stroke-linecap="round" stroke-linejoin="round"/>
</svg></div><div class="text-block-12">About us </div></div><nav class="dropdown-list w-dropdown-list"><div class="nav_drop-down-links"><div class="mega-menu_link-items-wrapper"><a href="/about" class="mega-menu_link-item">Our Story</a><a href="/faq" class="mega-menu_link-item">FAQ</a><a href="/contact" class="mega-menu_link-item">Contact us</a></div><div class="dropdown_top-colour is-green"></div></div></nav></div><a href="/publications" class="uui-navbar02_link-3 w-nav-link">Blog</a><a href="/pricing" class="uui-navbar02_link-3 w-nav-link">Pricing</a></div><div class="uui-navbar02_menu-right-3"><div class="uui-navbar02_button-wrapper-3"><a href="#" class="uui-button-secondary-gray-3 show-tablet w-inline-block"><div>Log in</div></a><a href="https://app.fruitpunch.ai/account/login" class="uui-button-tertiary-gray-3 hide-tablet w-inline-block"><div class="text-block-16">Log in</div></a><a href="https://app.fruitpunch.ai/account/register" class="button outline w-inline-block"><div>Get Started</div></a></div></div></nav><div class="uui-navbar02_menu-button-3 w-nav-button"><div class="menu-icon_component-3"><div class="menu-icon_line-top-3"></div><div class="menu-icon_line-middle-3"><div class="menu-icon_line-middle-inner-3"></div></div><div class="menu-icon_line-bottom-3"></div></div></div></div></div><a href="https://api.whatsapp.com/message/SO3LKMLLO5GZI1" target="_blank" class="whatsapp w-inline-block"><div class="list-icon-white margin w-embed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M.057 24l1.687-6.163c-1.041-1.804-1.588-3.849-1.587-5.946.003-6.556 5.338-11.891 11.893-11.891 3.181.001 6.167 1.24 8.413 3.488 2.245 2.248 3.481 5.236 3.48 8.414-.003 6.557-5.338 11.892-11.893 11.892-1.99-.001-3.951-.5-5.688-1.448l-6.305 1.654zm6.597-3.807c1.676.995 3.276 1.591 5.392 1.592 5.448 0 9.886-4.434 9.889-9.885.002-5.462-4.415-9.89-9.881-9.892-5.452 0-9.887 4.434-9.889 9.884-.001 2.225.651 3.891 1.746 5.634l-.999 3.648 3.742-.981zm11.387-5.464c-.074-.124-.272-.198-.57-.347-.297-.149-1.758-.868-2.031-.967-.272-.099-.47-.149-.669.149-.198.297-.768.967-.941 1.165-.173.198-.347.223-.644.074-.297-.149-1.255-.462-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.297-.347.446-.521.151-.172.2-.296.3-.495.099-.198.05-.372-.025-.521-.075-.148-.669-1.611-.916-2.206-.242-.579-.487-.501-.669-.51l-.57-.01c-.198 0-.52.074-.792.372s-1.04 1.016-1.04 2.479 1.065 2.876 1.213 3.074c.149.198 2.095 3.2 5.076 4.487.709.306 1.263.489 1.694.626.712.226 1.36.194 1.872.118.571-.085 1.758-.719 2.006-1.413.248-.695.248-1.29.173-1.414z" fill="currentColor"/></svg></div><h1 class="icontext-copy">Chat with our AI Expert</h1></a></div><div class="global-elements"><div class="html w-embed"><style>
/* Main Variables */
:root {
  --main-dark: #eb306e;
  --main-light: #fffffe;
}

/* Global Styles */

/* .body {
  font-feature-settings: 'cv08' on, 'salt' on, 'ss02' on, 'cv05' on, 'cv01' on, 'cv06' on, 'cv10' on, 'cv11' on;
}
*/

::selection {
	background: var(--main-dark);
  color: var(--main-light);
  text-shadow: none;
}
img::selection, svg::selection {
	background: transparent;
}

/* Disable / enable clicking on an element and its children  */
.no-click {
	pointer-events: none;
}
.can-click {
	pointer-events: auto;
}

/* Container Max Width 
.container {
  max-width: 1280px;
}
*/
/* Splide settings */
.splide button:disabled {
	opacity: 0.4;
}

.splide .splide__pagination { 
	display: none;
}


/* Image overflow-none and animation fix for Safari */
.blog-post_image-height {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.highlight {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.blog-youtube-video-embed {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

</style></div></div><section class="section"><div class="container is-blog-header"><div class="blog-header"><div class="blog-header_info-wrapper"><div class="blog_published-date">September 21, 2022</div></div><div class="blog-header_content-wrapper"><h1>The pains of classifying flooded forests in satellite data</h1><div class="blog-header_intro">A tricky detection use case - from weeks of data pre-processing to training 2 CNNs; and why the answer might be in infrared band data.</div></div><div class="blog-header_image-wrapper"><div class="blog-header_image-height"><img height="100" loading="lazy" alt="" src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6641d523ea67321016aa9c82_header.jpeg" class="blog-header_hero-img"/></div></div></div></div></section><section class="section"><div class="container is-blog-body"><div class="blog-body_wrapper"><div class="blog-body_content"><div class="blog_rich-text w-richtext"><h2>Too much water is destroying Latvian forests </h2>

<p>Forests cover 52% of Latvia’s territory, which makes it one of the most forested countries of the European Union. This translates to 3.8 million hectares of forest cover. There has been an increase of forest area since the beginning of the 20th century, from 27% to 52%, increasing the area of damaged forest from fires, bark beetles or floods correspondingly. Excess water is the 3rd most common cause for forest damage in Latvia. It occurs after heavy rainfall or is even caused by beaver dams that end up affecting the flow of water. </p>

<p>The goal of the <a href="/challenges/ai-for-earth-2-forest-health">AI for Earth 2 - Forest Health Challenge</a> was to detect flooded forests in 2 monitored areas in Latvia - 21157 hectares of Saldus and 2276 hectares of Kalsnava area.
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:729px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="729px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cba6a3721285a497823_1AoEmYfVIO-k32R9DOMCxDPneMubAVykoTYzgzEmXYMd-o0pxza08D4ToeJXcBfur988ytYLCXfv9PCB1RZp-8XPZGo6se43X78clmCmmprSVnLA9SpYjQo6hQGATO_9iFm_vAMcbfbHuhUpHB4U9RzgeIovNc5nn5iZBDt6LYiHSB63AvapAJ8ghA.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">The two areas of interest in Latvia. The large area on the left corresponds to the Saldus region, the small one to the right is the Kalsnava region.</figcaption></figure></p>

<h2>Training a computer vision model to see through the foliage from space  </h2>

<p>The goal for our team of AI engineers was to build a machine learning model that can detect excess water in forests from satellite data to alert forestry services. But is it possible to see wet forest ground through leaves from space? We assumed the real catch of this Challenge might be on the side of data more than in modelling. Did we find a dataset good enough to train detection models with sufficient accuracy? </p>

<h2>A wealth of remote sensing data</h2>

<p>The data available for this challenge were satellite images as well as field data provided by our Challenge partner <a href="https://www.forestradar.com/">ForestRadar</a> and the <a href="http://www.silava.lv/mainen/aboutus.aspx">State Forest Research Institute “Silava”</a>. The satellite imagery was taken from Sentinel-1 and Sentinel-2 satellites. We also had orthophoto imagery along with lidar data from drones at our disposal. Moreover, we got our hands on the Digital Terrain Model (DTM) and Canopy Height Model (CHM) for the areas of interest. </p>

<p>Vector-based radar data that defined our areas of interest and flooded areas for each year was used as ground truth. The images used were in .tif format to go with QGIS and FME programs, in later stages with Python for modelling.</p>

<p>Due to the limited time we had for the Challenge we had to select the dataset we’d focus on.  We decided to use the Sentinel and orthophoto images along with the radar data. The Sentinel images have a maximum of 10m resolution while the orthophoto images have a 40cm resolution, making them heavier and much more detailed. The latter do not have any problems with clouds unlike the Sentinel-2 images.
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:801px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="801px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cba3048d085452ceb4a_k6u8fLmXIJLl9voywSwaMcptAFjHow0ZPj7KlvmdPY1XWGTvD3S3qivwr--1DVNZFl-26vuCAU8MrN-3ioSL7Y1j0La0K8cigrgkG9qYVc4wPK2Mr28WHfOQoEgAUZWzbxN9KUN5NCqcEcroRBFjNdHpDrPK4mOjwMjC166d_TaQWiMS_nDGSaE_iA.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">Sentinel-2 image example</figcaption></figure><figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:487px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="487px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cbb220d2e0ae1399b61_SJwRy8FMZJGXcdiETFXw66jxEnSPZdmUF1Elpao_nRiYFxAEqdMHP7MnNWmJ8dJstA6ASIYClYYGva0k642G-aY9ibWXuk4Iy1JvM6-oOxhuB-3LBwUM8iF1yQS_k22dEu676JDWxJTd2xaODwF6yWudXoPtSe-xbyMn9QbO8EqidlCrWMeVMexyYg.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">Orthophoto image example</figcaption></figure></p>

<h2>GIS-based pre-processing</h2>

<p>The most time consuming part of the Challenge was the collection of appropriate imagery and the preparation of the data to be used in modelling. Damaged area polygons, such as the ones shown in the image below, were provided along with the year that they were flagged. 
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:1469px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="1469px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cba8711e869ee66e5e7_iwIOGCQqR1brz-yi2rvDV8BKhtPxPdXsaKrAKJWe53pq0lcJzUfwkWz358suC-ZAqxvRSRRzPURx9anmkYIVUj7WbtRfcq33LChiBzrb5JayUkisdUgsfX-d_gTFX6w8p0Axbps25j20ze6fQCv8WjwkQBcRDZA8p6kUPWFok8k-1TCxtTXr14PMjw.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">Polygons shown in brown represent damaged forest in the Saldus area in 2017.</figcaption></figure>
Time stamping the flooding events was crucial, without it wouldn’t be possible to find images ‘before’ and ‘after’ the flood event. Another hurdle we faced was the fact that Sentinel-2 images needed to have small amounts of cloud coverage and that had to be checked manually. These constraints took away time available for image collection and eventually, it reduced the amount of data for model training and validation. </p>

<p>Steps taken in data pre-processing of Sentinel-2 images:</p>

<ol>
<li>Split Saldus area of interest (AOI) in half (Saldus 1, Saldus 2), the produced Sentinel images would be otherwise too large. The first half of Saldus AOI was of size 1439x1203 pixels and the second half was of size 1442x1207.</li>
<li>Download of appropriate imagery from 2016 to 2021 for all AOIs. The images were downloaded through <a href="https://www.sentinel-hub.com/">Sentinel-hub</a> and saved in <a href="https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/natural_color/">Natural Color</a>, with the 3 corresponding RGB bands.</li>
<li>Creation of masks for damaged areas for each year.</li>
<li>Upload of each image (in .tif format) and its corresponding mask (in .npy format).</li>
</ol>

<p><figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:1258px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="1258px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b105da5fe202c1ededb66_Screenshot%202022-09-21%20at%2015.22.51.png" loading="lazy" id="" width="100%" height="auto"></div><figcaption id="">Half of Saldus AOI on the left and the mask created for the same area, yellow pixels indicate flooded forest</figcaption></figure>
Steps taken in data pre-processing of orthophoto images:</p>

<ol>
<li>The area of interest was divided into 79 tiles. To reduce the weight of the dataset we only kept the tiles that contained any flood damage, as shown in the image below. This reduced the number of tiles to 51.</li>
<li>The images that were saved had four bands, RGB and Near-Infrared. Since they were 6250 x 6250 pixels big, we created smaller chips of the images.</li>
<li>We created and uploaded the 224 x 224 px images of healthy and damaged forests to be used as model inputs.</li>
</ol>

<p><figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:587px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="587px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cbbaf13b178a5c4bc5c_zhzIlcVoVxgY5J9yXtnycrZixENVp9HlaSsIPkliG0B7FocCbdEcj27SCxCTt2onpX8yaXZiGGsLL_n1cHzSr7TPk31MutOpCiyrh7DWj8uWjEuEGIk2BVTNMnVJBQtvzgbbSqhYIVfrQvStrZb6DMJ8BRr6VOV6UYD5c66W-ieLtYVCMFCzwo8xgQ.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">Orthophoto images for tiles with forest damage</figcaption></figure></p>

<h4>Flood inundation map for Saldus</h4>

<p>This is where radar stepped in to serve as ground truth. Sentinel-1 SAR GRD data was used for the Saldus area of interest. We followed these steps in the process:</p>

<ol>
<li>Each Sentinel-1 image downloaded had 3 bands: VV (vertical/vertical), VH (vertical/horizontal) and angle.</li>
<li>We applied a speckle filter to reduce noise (RefinedLee filter).</li>
<li>We applied a threshold to create a difference image by dividing after and before images. The threshold was of value 1.25.</li>
<li>We masked out pixels that have permanent water for more than 5 months by using Global Surface Water (GSW) dataset.</li>
<li>We’ve created masks of an area with a slope greater than 5 degrees.</li>
</ol>

<p>The resulting image of a calculated flooded area for Saldus AOI is shown below.
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:1600px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="1600px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cbb7a391c759bfbf6de_CmbszOW_TNx2-s19wzj2zvBfIyJ80Z1Fj7WwhjvjJ8bqbY7tYsEHPE8neEjvFqDnjPREO3bdUOK1GmBWyUycscsKhSpU4eNgZjVpyb9uT25ceLMDIBJdT9I79ykKFSlqu_Fj66zpvZT2EMzh4BG73nmT2OT7YMyNU1gI3cih3If4c-cSpZFWwhR-UA.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">A flood inundation map of Saldus AOI using Sentinel 1 SAR GRD data for 2018</figcaption></figure></p>

<h2>Using convolutional neural networks to solve the problem </h2>

<p>We wanted to classify the pixels of our images in one of two classes - ‘Flood’ or ‘No Flood’. We decided to use 3 models to solve this problem. </p>

<ul>
<li>XGBoost algorithm served as baseline</li>
<li>U-Net CNN</li>
<li>Inception-v4 CNN</li>
</ul>

<p>Both CNNs were pre-trained in the Image-net dataset. The desired result of our models would be to correctly classify each pixel as flooded or not.</p>

<h4>XGBoost</h4>

<p><a href="https://doi.org/10.1145/2939672.2939785">Extreme Gradient Boosting</a>is a supervised machine learning algorithm used to train a model in order to find patterns in a dataset with labels and features. This type of algorithm is not meant to be used for image classification. In contrast to a CNN, it takes each piece of data as independent. Nevertheless, we used it to classify each pixel individually and took it as a benchmark for the rest of our models. We trained the XGBoost with the Sentinel-2 images and the corresponding masks to obtain the classification for each pixel as flooded or non-flooded.</p>

<h4>U-Net</h4>

<p><a href="https://arxiv.org/abs/1505.04597">U-Net</a> is a <strong>Convolutional Neural Network</strong> (CNN) that was developed for biomedical image segmentation. The network consists of a contracting and an expansive path, which give it the U-shaped architecture. The U-Net is meant to solve pixel-wise classification problems, segmenting the image and classifying each part. The prediction is a mask for the image. We first trained the U-Net with the Sentinel-2 images and the corresponding masks and later tried the same with the orthophoto images. </p>

<h4>Inception-v4</h4>

<p>An <a href="https://arxiv.org/abs/1602.07261">Inception network</a> is a <strong>Convolutional Neural Network</strong> (CNN) which consists of repeating patterns of convolutional design configurations called Inception modules. It has the ability to extract features from data of varying scales through the utilisation of varying convolutional layers. This CNN classifies the entire image, appointing one label to each image.</p>

<p>At first, we didn’t think about using this type of model since we thought it worked better with semantic segmentation. But since the performance of the U-Net on the orthophoto dataset was low; we wanted to try out a different approach. We believe this could be attributed to the resolution of the images, showing only a small portion of the surface. </p>

<p>To create images and masks suitable for the CNNs, subset images were created. For each AOI and for each year, subset images and their corresponding masks were created around a damaged area. This was represented by a polygon as can be seen in the figure below.
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:756px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="756px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0cbb8711e83bb766e605_58gkuNeZRNaKE8Pcf6SOY5D46CPJXUn7VMARZzEZwlE_nEPoqqOWVuVy4W5nInWH5Jdj3Jg0-658b6Sy4az6_Hr5MI1-8Fo5HKPH7EmgwtKpcmE4dE_fHESU55VzJL159v20HKPw2RqzNJ_K9y-Uqc0P0tCNwCEADBzjULlbVqsP4HGdAoQVlH9Rhw.png" id="" width="100%" height="auto" loading="auto"></div><figcaption id="">Damaged forest polygons in a Saldus area image</figcaption></figure>
The same procedure was applied to the orthophoto images. Subset images of 224x224 px size were created for the model input. For the Inception-v4, some of the orthophoto images were separated into “damaged” and “healthy”, since this model does not need masks.</p>

<h2>Model results showed the need for exploring further paths</h2>

<p>The evaluation metrics we used were:</p>

<ul>
<li>Intersection Over Union (IOU)</li>
<li>Confusion matrix</li>
<li>Classification accuracy of the model</li>
</ul>

<p>The results derived from the models are presented in the tables below. 
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:1216px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="1216px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0f0061ecf41d8184742f_Screenshot%202022-09-21%20at%2015.16.38.png" loading="lazy" id="" width="100%" height="auto"></div><figcaption id="">Sentinel-2 dataset results</figcaption></figure>
For the Sentinel-2 data, we started setting our benchmark with the XGBoost model getting around 66-67% classification accuracy. This model was not meant for image classification, it takes each pixel independently and classifies if as flooded or not flooded. The U-Net improved the performance with a 71% IOU.
<figure id="" class="w-richtext-figure-type-image w-richtext-align-fullwidth" style="max-width:1214px" data-rt-type="image" data-rt-align="fullwidth" data-rt-max-width="1214px"><div id=""><img src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/632b0f13f64aa314029aba96_Screenshot%202022-09-21%20at%2015.17.38.png" loading="lazy" id="" width="100%" height="auto"></div><figcaption id="">Orthophoto dataset results</figcaption></figure>
When we trained the U-Net with the orthophoto images, we obtained a significantly poorer performance (55% IOU) than with the Sentinel-2 images. With the Inception-v4 the performance improved, getting an accuracy of 74%. It’s important to mention here that the dataset was balanced but relatively small. Increasing the amount of images would get us a more precise result.</p>

<h2>Improving performance with near-infrared band</h2>

<p>Due to time constraints during the AI for Forest Health Challenge, we could only use satellite images that had 3 bands (RGB). A promising follow-up would be to add a <strong>Normalised Difference Water Index (NDWI)</strong>, which is a satellite-derived index from the <strong>Near-Infrared (NIR)</strong> and <strong>Short Wave Infrared (SWIR)</strong> channels. Near infrared band is reflected by vegetation. This index is strongly related to the plant water content, which makes it a very good proxy for plant water stress. Another layer to be added could be a <strong>Normalised Difference Vegetation Index (NDVI),</strong> which quantifies vegetation by measuring the difference between near-infrared and red light (which vegetation absorbs). </p>

<p>Another possible approach could be to limit the training of the algorithms to forested areas with floods, i.e. to areas that do not have other types of flooded terrain.</p>

<h2>Not seeing the forest for the trees</h2>

<p>Literally. Identifying flooded areas under tree foliage proved to be very difficult. We’ve also learned that working with geospatial data poses many challenges - from the manual inspection of images all the way to the size of the whole dataset. In this challenge we did our best to acquire suitable satellite and orthophoto images in order to classify their pixels as flooded or not. The results show some promise, especially considering that more images could always be added to the model training phase, as well as more epochs to the CNNs.</p>

<p>We hope the Challenge will be followed up with some of the suggested next steps and experimentation. Monitoring the changes in Latvian forests is crucial for their health. Especially since human activities and climate change have stressed the ecosystem even more and made the preservation of forests and wildlife more difficult. </p>

<p><em>*Enias Vodas &amp; Julieta Millán *</em></p>

<p>AI for Earth engineers </p>

<p><em>AI for Forest Health Team:  Deepali Bidwai, Mohammad Alasawdah, Tim De Craecker, Julieta Millán, Enias Vodas</em></p>
</div><div class="w-dyn-list"><div role="list" class="collection-list w-dyn-items"><div role="listitem" class="collection-item w-dyn-item"><div class="tag">AI for Earth</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Remote Sensing</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">GIS</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Challenge results</div></div></div></div></div><div class="blog-body_social"><div class="blog_right-col-sticky-wrapper"><div class="video-2 w-dyn-bind-empty w-video w-embed"></div><div class="blog-body_social-wrapper"><div class="blog-body_social-content"><div class="blog-social_icon-wrapper"><div class="blog_social-heading">Subscribe to our newsletter</div><div class="icon-embed _32x32 w-embed"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg">
<circle cx="16" cy="16" r="16" fill="#FDEAF1"/>
<path d="M21 25.25H11C10.59 25.25 10.25 24.91 10.25 24.5C10.25 24.09 10.59 23.75 11 23.75H21C23.86 23.75 25.25 22.36 25.25 19.5V12.5C25.25 9.64 23.86 8.25 21 8.25H11C8.14 8.25 6.75 9.64 6.75 12.5C6.75 12.91 6.41 13.25 6 13.25C5.59 13.25 5.25 12.91 5.25 12.5C5.25 8.85 7.35 6.75 11 6.75H21C24.65 6.75 26.75 8.85 26.75 12.5V19.5C26.75 23.15 24.65 25.25 21 25.25Z" fill="#EB306E"/>
<path d="M15.9998 16.87C15.1598 16.87 14.3098 16.61 13.6598 16.08L10.5298 13.58C10.2098 13.32 10.1498 12.85 10.4098 12.53C10.6698 12.21 11.1398 12.15 11.4598 12.41L14.5898 14.91C15.3498 15.52 16.6398 15.52 17.3998 14.91L20.5298 12.41C20.8498 12.15 21.3298 12.2 21.5798 12.53C21.8398 12.85 21.7898 13.33 21.4598 13.58L18.3298 16.08C17.6898 16.61 16.8398 16.87 15.9998 16.87Z" fill="#EB306E"/>
<path d="M12 21.25H6C5.59 21.25 5.25 20.91 5.25 20.5C5.25 20.09 5.59 19.75 6 19.75H12C12.41 19.75 12.75 20.09 12.75 20.5C12.75 20.91 12.41 21.25 12 21.25Z" fill="#EB306E"/>
<path d="M9 17.25H6C5.59 17.25 5.25 16.91 5.25 16.5C5.25 16.09 5.59 15.75 6 15.75H9C9.41 15.75 9.75 16.09 9.75 16.5C9.75 16.91 9.41 17.25 9 17.25Z" fill="#EB306E"/>
</svg></div><p class="paragraph">Be the first to know when a new AI for Good challenge is launched. Keep up do date with the latest AI for Good news. </p></div></div><div class="blog-body_newsletter-wrapper"><div id="newsletter-form" data-w-id="f0d3febe-68b2-c3a3-e159-9a6fdebd0168" class="blog_newsletter-form-wrapper w-form"><form id="wf-form-EMAIL" name="wf-form-EMAIL" data-name="EMAIL" action="https://fruitpunch.us20.list-manage.com/subscribe/post?u=274b78179f279b8ff0a6f8164&amp;amp;id=b25b3d594e" method="post" class="blog_newsletter-form" data-wf-page-id="661d09a4061bc8f0cc423c89" data-wf-element-id="f0d3febe-68b2-c3a3-e159-9a6fdebd0169"><div class="blog_newsletter-input-wrapper"><div class="w-embed w-script"><div id="mc_embed_shell">
      <link href="" rel="stylesheet" type="text/css">
  <style type="text/css">
        #mc_embed_signup{background:#f7f8f8; false;clear:left; font:1em; font-weight:600; Inter, sans-serif; width: 100%;}
        /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
           We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
    <form action="https://fruitpunch.us20.list-manage.com/subscribe/post?u=274b78179f279b8ff0a6f8164&amp;id=b25b3d594e&amp;f_id=00207ee4f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank">
        <div id="mc_embed_signup_scroll">
            <div class="indicates-required" style="color:#000000;float:right;font-weight:300"><span class="asterisk">*</span> indicates required</div>
            <div class="mc-field-group"><label for="mce-EMAIL" style="color:#000000;margin-bottom:5px;display:block">Email Address <span class="asterisk">*</span></label><input type="email" name="EMAIL" class="input-field w-input required email" id="mce-EMAIL" required="" value=""></div>
        <div id="mce-responses" class="clear" style="color:#000000;font-weight:300;font-style:italic">
            <div class="response" id="mce-error-response" style="display: none;"></div>
            <div class="response" id="mce-success-response" style="display: none;"></div>
        </div><div aria-hidden="true" style="position: absolute; left: -5000px;"><input type="text" name="b_274b78179f279b8ff0a6f8164_b25b3d594e" tabindex="-1" value=""></div><div class="clear"><input type="submit" name="subscribe" id="mc-embedded-subscribe" style="margin-top:5px" class="button outline" value="Subscribe"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='MMERGE1';ftypes[1]='phone';fnames[2]='MMERGE2';ftypes[2]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></div></div></div></form><div class="success-message w-form-done"><div class="success-message_heading">Thank you!</div><p class="success-message_sub-heading">We’ve just sent you a confirmation email.</p><p class="success-message_p">We know, this can be annoying, but we want to make sure we don’t spam anyone. Please, check out your inbox and confirm the link in the email.<br/><br/>Once confirmed, you’ll be ready to go!</p></div><div class="error-message w-form-fail"><div class="error-message-p">Oops! Something went wrong while submitting the form.</div></div></div></div></div><div class="blog_next-item-wrapper"></div></div></div></div></div></section><section class="section related"><div class="container is-sm-padding is-btm related"><h3>You may also like</h3><div class="w-dyn-list"><div role="list" class="w-dyn-items w-row"><div role="listitem" class="w-dyn-item w-col w-col-6"><a data-w-id="273a5006-de60-6f4f-9030-607401257248" href="/blog/prioritizing-essential-care-with-ai" class="blog_post-card w-inline-block"><div class="blog-post_image-wrapper"><div class="blog-post_image-height"><img height="100" loading="lazy" style="-webkit-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-moz-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-ms-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0)" src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6798efc44097af4454e6f6f8_header.jpeg" alt="" class="blog-post_img"/></div></div><div class="blog-post_content-wrapper"><div class="blog-post_date">January 28, 2025</div><h3>Prioritizing Essential Care with AI</h3><p class="blog-page_post-p">Advancing Neonatal Care: The Role of IMPALA and AI in Improving Early Diagnosis and Treatment</p><address fs-cmsnest-collection="categories"></address></div></a></div><div role="listitem" class="w-dyn-item w-col w-col-6"><a data-w-id="273a5006-de60-6f4f-9030-607401257248" href="/blog/the-bear-necessity-of-ai-in-conservation" class="blog_post-card w-inline-block"><div class="blog-post_image-wrapper"><div class="blog-post_image-height"><img height="100" loading="lazy" style="-webkit-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-moz-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-ms-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0)" src="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6643611db64f259e60a5aaca_header.jpeg" alt="" class="blog-post_img"/></div></div><div class="blog-post_content-wrapper"><div class="blog-post_date">May 14, 2024</div><h3>The Bear Necessity of AI in Conservation</h3><p class="blog-page_post-p">The AI for Bears Challenge results which aims to improve the monitoring and identification of bears using advanced computer vision techniques.</p><address fs-cmsnest-collection="categories"></address></div></a></div></div></div></div></section><div class="global-elements"><div class="html w-embed"><style>
/* Main Variables */
:root {
  --main-dark: #eb306e;
  --main-light: #fffffe;
}

/* Global Styles */

/* .body {
  font-feature-settings: 'cv08' on, 'salt' on, 'ss02' on, 'cv05' on, 'cv01' on, 'cv06' on, 'cv10' on, 'cv11' on;
}
*/

::selection {
	background: var(--main-dark);
  color: var(--main-light);
  text-shadow: none;
}
img::selection, svg::selection {
	background: transparent;
}

/* Disable / enable clicking on an element and its children  */
.no-click {
	pointer-events: none;
}
.can-click {
	pointer-events: auto;
}

/* Container Max Width 
.container {
  max-width: 1280px;
}
*/
/* Splide settings */
.splide button:disabled {
	opacity: 0.4;
}

.splide .splide__pagination { 
	display: none;
}


/* Image overflow-none and animation fix for Safari */
.blog-post_image-height {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.highlight {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.blog-youtube-video-embed {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

</style></div></div><footer class="uui-footer03_component-2"><div class="uui-page-padding-5"><div class="uui-container-large-7"><div class="uui-padding-vertical-xlarge-2"><div class="w-layout-grid uui-footer03_top-wrapper-2"><div class="uui-footer03_left-wrapper-2"><a href="#" class="uui-footer03_logo-link-2 w-nav-brand"><div class="uui-logo_component-5"><img src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/624fffdc005e2601af3e69d2_Fp-logo-white.svg" loading="lazy" alt="Untitled UI logotext" class="uui-logo_logotype-4"/><img src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/652797dd2601a8abb837b8f7_untitled-ui-logo.png" loading="lazy" alt="Logo" class="uui-logo_image-4"/></div></a><div class="uui-footer03_details-wrapper-2"><div class="uui-text-size-medium-4">Train your AI skills and achieve your learning goals by solving real-world Challenges mentored by experts.</div></div><a href="https://app.fruitpunch.ai/account/register" class="button w-button">Start making impact</a><div class="w-layout-grid uui-footer03_social-list-2"><a href="https://www.facebook.com/FruitPunchAI/" target="_blank" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M24 12C24 5.37258 18.6274 0 12 0C5.37258 0 0 5.37258 0 12C0 17.9895 4.3882 22.954 10.125 23.8542V15.4688H7.07812V12H10.125V9.35625C10.125 6.34875 11.9166 4.6875 14.6576 4.6875C15.9701 4.6875 17.3438 4.92188 17.3438 4.92188V7.875H15.8306C14.34 7.875 13.875 8.80008 13.875 9.75V12H17.2031L16.6711 15.4688H13.875V23.8542C19.6118 22.954 24 17.9895 24 12Z" fill="currentColor"/>
</svg></div></a><a href="https://www.instagram.com/fruitpunchai/" target="_blank" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 2.16094C15.2063 2.16094 15.5859 2.175 16.8469 2.23125C18.0188 2.28281 18.6516 2.47969 19.0734 2.64375C19.6313 2.85938 20.0344 3.12188 20.4516 3.53906C20.8734 3.96094 21.1313 4.35938 21.3469 4.91719C21.5109 5.33906 21.7078 5.97656 21.7594 7.14375C21.8156 8.40937 21.8297 8.78906 21.8297 11.9906C21.8297 15.1969 21.8156 15.5766 21.7594 16.8375C21.7078 18.0094 21.5109 18.6422 21.3469 19.0641C21.1313 19.6219 20.8688 20.025 20.4516 20.4422C20.0297 20.8641 19.6313 21.1219 19.0734 21.3375C18.6516 21.5016 18.0141 21.6984 16.8469 21.75C15.5813 21.8063 15.2016 21.8203 12 21.8203C8.79375 21.8203 8.41406 21.8063 7.15313 21.75C5.98125 21.6984 5.34844 21.5016 4.92656 21.3375C4.36875 21.1219 3.96563 20.8594 3.54844 20.4422C3.12656 20.0203 2.86875 19.6219 2.65313 19.0641C2.48906 18.6422 2.29219 18.0047 2.24063 16.8375C2.18438 15.5719 2.17031 15.1922 2.17031 11.9906C2.17031 8.78438 2.18438 8.40469 2.24063 7.14375C2.29219 5.97187 2.48906 5.33906 2.65313 4.91719C2.86875 4.35938 3.13125 3.95625 3.54844 3.53906C3.97031 3.11719 4.36875 2.85938 4.92656 2.64375C5.34844 2.47969 5.98594 2.28281 7.15313 2.23125C8.41406 2.175 8.79375 2.16094 12 2.16094ZM12 0C8.74219 0 8.33438 0.0140625 7.05469 0.0703125C5.77969 0.126563 4.90313 0.332812 4.14375 0.628125C3.35156 0.9375 2.68125 1.34531 2.01563 2.01562C1.34531 2.68125 0.9375 3.35156 0.628125 4.13906C0.332812 4.90313 0.126563 5.775 0.0703125 7.05C0.0140625 8.33437 0 8.74219 0 12C0 15.2578 0.0140625 15.6656 0.0703125 16.9453C0.126563 18.2203 0.332812 19.0969 0.628125 19.8563C0.9375 20.6484 1.34531 21.3188 2.01563 21.9844C2.68125 22.65 3.35156 23.0625 4.13906 23.3672C4.90313 23.6625 5.775 23.8687 7.05 23.925C8.32969 23.9812 8.7375 23.9953 11.9953 23.9953C15.2531 23.9953 15.6609 23.9812 16.9406 23.925C18.2156 23.8687 19.0922 23.6625 19.8516 23.3672C20.6391 23.0625 21.3094 22.65 21.975 21.9844C22.6406 21.3188 23.0531 20.6484 23.3578 19.8609C23.6531 19.0969 23.8594 18.225 23.9156 16.95C23.9719 15.6703 23.9859 15.2625 23.9859 12.0047C23.9859 8.74688 23.9719 8.33906 23.9156 7.05938C23.8594 5.78438 23.6531 4.90781 23.3578 4.14844C23.0625 3.35156 22.6547 2.68125 21.9844 2.01562C21.3188 1.35 20.6484 0.9375 19.8609 0.632812C19.0969 0.3375 18.225 0.13125 16.95 0.075C15.6656 0.0140625 15.2578 0 12 0Z" fill="currentColor"/>
<path d="M12 5.83594C8.59688 5.83594 5.83594 8.59688 5.83594 12C5.83594 15.4031 8.59688 18.1641 12 18.1641C15.4031 18.1641 18.1641 15.4031 18.1641 12C18.1641 8.59688 15.4031 5.83594 12 5.83594ZM12 15.9984C9.79219 15.9984 8.00156 14.2078 8.00156 12C8.00156 9.79219 9.79219 8.00156 12 8.00156C14.2078 8.00156 15.9984 9.79219 15.9984 12C15.9984 14.2078 14.2078 15.9984 12 15.9984Z" fill="currentColor"/>
<path d="M19.8469 5.59214C19.8469 6.38902 19.2 7.0312 18.4078 7.0312C17.6109 7.0312 16.9688 6.38433 16.9688 5.59214C16.9688 4.79526 17.6156 4.15308 18.4078 4.15308C19.2 4.15308 19.8469 4.79995 19.8469 5.59214Z" fill="currentColor"/>
</svg></div></a><a href="https://twitter.com/fruitpunchai?s=20&amp;t=r8C8DvQEjIrOl24_F6r5FQ" target="_blank" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" aria-hidden="true" class="r-13v1u17 r-4qtqp9 r-yyyyoo r-16y2uox r-8kz0gk r-dnmrzs r-bnwqim r-1plcrui r-lrvibr r-lrsllp"><g><path d="M14.258 10.152L23.176 0h-2.113l-7.747 8.813L7.133 0H0l9.352 13.328L0 23.973h2.113l8.176-9.309 6.531 9.309h7.133zm-2.895 3.293l-.949-1.328L2.875 1.56h3.246l6.086 8.523.945 1.328 7.91 11.078h-3.246zm0 0" fill="CurrentColor"></path></g></svg></div></a><a href="https://www.linkedin.com/company/fruitpunchai/" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M22.2234 0H1.77187C0.792187 0 0 0.773438 0 1.72969V22.2656C0 23.2219 0.792187 24 1.77187 24H22.2234C23.2031 24 24 23.2219 24 22.2703V1.72969C24 0.773438 23.2031 0 22.2234 0ZM7.12031 20.4516H3.55781V8.99531H7.12031V20.4516ZM5.33906 7.43438C4.19531 7.43438 3.27188 6.51094 3.27188 5.37187C3.27188 4.23281 4.19531 3.30937 5.33906 3.30937C6.47813 3.30937 7.40156 4.23281 7.40156 5.37187C7.40156 6.50625 6.47813 7.43438 5.33906 7.43438ZM20.4516 20.4516H16.8937V14.8828C16.8937 13.5562 16.8703 11.8453 15.0422 11.8453C13.1906 11.8453 12.9094 13.2937 12.9094 14.7891V20.4516H9.35625V8.99531H12.7687V10.5609H12.8156C13.2891 9.66094 14.4516 8.70938 16.1813 8.70938C19.7859 8.70938 20.4516 11.0813 20.4516 14.1656V20.4516Z" fill="currentColor"/>
</svg></div></a></div></div><div id="w-node-_3c7e8777-b0d9-0932-e324-81bb1cd3af93-1cd3af7b" class="w-layout-grid uui-footer03_menu-wrapper-2"><div class="uui-footer03_link-list-2"><a href="https://app.fruitpunch.ai/challenges" class="uui-footer03_link-2 w-inline-block"><div class="text-block-13">Challenges</div></a><a href="/about" class="uui-footer03_link-2 w-inline-block"><div class="text-block-14">About us</div></a><a href="/faq" class="uui-footer03_link-2 text-block-14 w-inline-block"><div>FAQ</div><div class="uui-badge-small-success-4"><div>New</div></div></a><a href="/contact" class="uui-footer03_link-2 w-inline-block"><div>Contact us</div></a><a href="/publications" class="uui-footer03_link-2 w-inline-block"><div>Blog</div></a><a href="/pricing" class="uui-footer03_link-2 w-inline-block"><div>Pricing</div></a></div><div class="uui-footer03_link-list-2"><a href="#" class="uui-footer03_link-2-copy w-inline-block"><div>Our Labs</div></a><a href="/labs/ai-for-wildlife-lab" class="uui-footer03_link-2 w-inline-block"><div>AI for Wildlife Lab</div></a><a href="/labs/ai-for-earth-lab" class="uui-footer03_link-2 w-inline-block"><div>AI for Earth Lab</div></a><a href="/labs/ai-for-health-lab" class="uui-footer03_link-2 w-inline-block"><div>AI for Health Lab</div></a><a href="#" class="uui-footer03_link-2 w-inline-block"></a><a href="#" class="uui-footer03_link-2 w-inline-block"></a><a href="#" class="uui-footer03_link-2 w-inline-block"></a></div></div></div><div class="uui-footer03_bottom-wrapper-2"><div class="uui-text-size-small-4 text-color-gray500">Copyright © 2023 FruitPunch AI B.V. All rights reserved.</div><div class="w-layout-grid uui-footer03_legal-list-2"><a href="/terms-of-use" class="uui-footer03_legal-link-2">Terms of Use</a><a href="/legal-privacy" class="uui-footer03_legal-link-2">Privacy Policy</a><a href="#" class="uui-footer03_legal-link-2">Cookies</a></div></div></div></div></div></footer><script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=623d7d246e8213dbe6aace84" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script><script src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/js/webflow.schunk.b7cad701f94860c2.js" type="text/javascript"></script><script src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/js/webflow.schunk.bb3b2bf26788d119.js" type="text/javascript"></script><script src="https://cdn.prod.website-files.com/623d7d246e8213dbe6aace84/js/webflow.9d5bcaf8.33261154d2049663.js" type="text/javascript"></script><!--Tooltip Scripts & Settings-->
<script src="https://unpkg.com/popper.js@1"></script>
<script src="https://unpkg.com/tippy.js@4"></script>
<script>
tippy('.tooltip', {        
 animation: 'fade',    
 duration: 200,      
 arrow: true,          
 delay: [0, 50],      
 arrowType: 'sharp',  
 theme: 'light-border',        
 maxWidth: 220,    
 interactive: true,
})
</script>
<script type="text/javascript">
_linkedin_partner_id = "4021729";
window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
(function(l) {
if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
window.lintrk.q=[]}
var s = document.getElementsByTagName("script")[0];
var b = document.createElement("script");
b.type = "text/javascript";b.async = true;
b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
s.parentNode.insertBefore(b, s);})(window.lintrk);
</script>
<noscript>
<img height="1" width="1" style="display:none;" alt="" src="https://px.ads.linkedin.com/collect/?pid=4021729&fmt=gif" />
</noscript></body></html>