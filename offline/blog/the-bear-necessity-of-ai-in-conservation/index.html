<html data-wf-domain="www.fruitpunch.ai" data-wf-page="661d09a4061bc8f0cc423c89" data-wf-site="623d7d246e8213dbe6aace84" data-wf-collection="661d09a4061bc8f0cc423c7d" data-wf-item-slug="the-bear-necessity-of-ai-in-conservation" class="w-mod-js wf-atkinsonhyperlegible-n4-active wf-atkinsonhyperlegible-n7-active wf-robotomono-n6-active wf-robotomono-n4-active wf-robotomono-n7-active wf-inter-n5-active wf-inter-n6-active wf-inter-n3-active wf-inter-n2-active wf-inter-n1-active wf-inter-n4-active wf-inter-n7-active wf-inter-n9-active wf-inter-n8-active wf-active w-mod-ix"><head><style>.wf-force-outline-none[tabindex="-1"]:focus{outline:none;}</style><meta charset="utf-8"><title>The Bear Necessity of AI in Conservation</title><meta content="The AI for Bears Challenge results which aims to improve the monitoring and identification of bears using advanced computer vision techniques.May 14, 2024" name="description"><meta content="The Bear Necessity of AI in Conservation" property="og:title"><meta content="The AI for Bears Challenge results which aims to improve the monitoring and identification of bears using advanced computer vision techniques.May 14, 2024" property="og:description"><meta content="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6643611db64f259e60a5aaca_header.jpeg" property="og:image"><meta content="The Bear Necessity of AI in Conservation" property="twitter:title"><meta content="The AI for Bears Challenge results which aims to improve the monitoring and identification of bears using advanced computer vision techniques.May 14, 2024" property="twitter:description"><meta content="https://cdn.prod.website-files.com/62604c2173fafef5f182b55a/6643611db64f259e60a5aaca_header.jpeg" property="twitter:image"><meta property="og:type" content="website"><meta content="summary_large_image" name="twitter:card"><meta content="width=device-width, initial-scale=1" name="viewport"><meta content="3W8ATyTMYMESrts8sQd9lyAvrN8Svo_I9Ks_ZE2td3I" name="google-site-verification"><style type="text/css" data-tippy-stylesheet="">.tippy-iOS{cursor:pointer!important;-webkit-tap-highlight-color:transparent}.tippy-popper{transition-timing-function:cubic-bezier(.165,.84,.44,1);max-width:calc(100% - 8px);pointer-events:none;outline:0}.tippy-popper[x-placement^=top] .tippy-backdrop{border-radius:40% 40% 0 0}.tippy-popper[x-placement^=top] .tippy-roundarrow{bottom:-7px;bottom:-6.5px;-webkit-transform-origin:50% 0;transform-origin:50% 0;margin:0 3px}.tippy-popper[x-placement^=top] .tippy-roundarrow svg{position:absolute;left:0;-webkit-transform:rotate(180deg);transform:rotate(180deg)}.tippy-popper[x-placement^=top] .tippy-arrow{border-top:8px solid #333;border-right:8px solid transparent;border-left:8px solid transparent;bottom:-7px;margin:0 3px;-webkit-transform-origin:50% 0;transform-origin:50% 0}.tippy-popper[x-placement^=top] .tippy-backdrop{-webkit-transform-origin:0 25%;transform-origin:0 25%}.tippy-popper[x-placement^=top] .tippy-backdrop[data-state=visible]{-webkit-transform:scale(1) translate(-50%,-55%);transform:scale(1) translate(-50%,-55%)}.tippy-popper[x-placement^=top] .tippy-backdrop[data-state=hidden]{-webkit-transform:scale(.2) translate(-50%,-45%);transform:scale(.2) translate(-50%,-45%);opacity:0}.tippy-popper[x-placement^=top] [data-animation=shift-toward][data-state=visible]{-webkit-transform:translateY(-10px);transform:translateY(-10px)}.tippy-popper[x-placement^=top] [data-animation=shift-toward][data-state=hidden]{opacity:0;-webkit-transform:translateY(-20px);transform:translateY(-20px)}.tippy-popper[x-placement^=top] [data-animation=perspective]{-webkit-transform-origin:bottom;transform-origin:bottom}.tippy-popper[x-placement^=top] [data-animation=perspective][data-state=visible]{-webkit-transform:perspective(700px) translateY(-10px);transform:perspective(700px) translateY(-10px)}.tippy-popper[x-placement^=top] [data-animation=perspective][data-state=hidden]{opacity:0;-webkit-transform:perspective(700px) rotateX(60deg);transform:perspective(700px) rotateX(60deg)}.tippy-popper[x-placement^=top] [data-animation=fade][data-state=visible]{-webkit-transform:translateY(-10px);transform:translateY(-10px)}.tippy-popper[x-placement^=top] [data-animation=fade][data-state=hidden]{opacity:0;-webkit-transform:translateY(-10px);transform:translateY(-10px)}.tippy-popper[x-placement^=top] [data-animation=shift-away][data-state=visible]{-webkit-transform:translateY(-10px);transform:translateY(-10px)}.tippy-popper[x-placement^=top] [data-animation=shift-away][data-state=hidden]{opacity:0}.tippy-popper[x-placement^=top] [data-animation=scale]{-webkit-transform-origin:bottom;transform-origin:bottom}.tippy-popper[x-placement^=top] [data-animation=scale][data-state=visible]{-webkit-transform:translateY(-10px);transform:translateY(-10px)}.tippy-popper[x-placement^=top] [data-animation=scale][data-state=hidden]{opacity:0;-webkit-transform:translateY(-10px) scale(.5);transform:translateY(-10px) scale(.5)}.tippy-popper[x-placement^=bottom] .tippy-backdrop{border-radius:0 0 30% 30%}.tippy-popper[x-placement^=bottom] .tippy-roundarrow{top:-7px;-webkit-transform-origin:50% 100%;transform-origin:50% 100%;margin:0 3px}.tippy-popper[x-placement^=bottom] .tippy-roundarrow svg{position:absolute;left:0}.tippy-popper[x-placement^=bottom] .tippy-arrow{border-bottom:8px solid #333;border-right:8px solid transparent;border-left:8px solid transparent;top:-7px;margin:0 3px;-webkit-transform-origin:50% 100%;transform-origin:50% 100%}.tippy-popper[x-placement^=bottom] .tippy-backdrop{-webkit-transform-origin:0 -50%;transform-origin:0 -50%}.tippy-popper[x-placement^=bottom] .tippy-backdrop[data-state=visible]{-webkit-transform:scale(1) translate(-50%,-45%);transform:scale(1) translate(-50%,-45%)}.tippy-popper[x-placement^=bottom] .tippy-backdrop[data-state=hidden]{-webkit-transform:scale(.2) translate(-50%);transform:scale(.2) translate(-50%);opacity:0}.tippy-popper[x-placement^=bottom] [data-animation=shift-toward][data-state=visible]{-webkit-transform:translateY(10px);transform:translateY(10px)}.tippy-popper[x-placement^=bottom] [data-animation=shift-toward][data-state=hidden]{opacity:0;-webkit-transform:translateY(20px);transform:translateY(20px)}.tippy-popper[x-placement^=bottom] [data-animation=perspective]{-webkit-transform-origin:top;transform-origin:top}.tippy-popper[x-placement^=bottom] [data-animation=perspective][data-state=visible]{-webkit-transform:perspective(700px) translateY(10px);transform:perspective(700px) translateY(10px)}.tippy-popper[x-placement^=bottom] [data-animation=perspective][data-state=hidden]{opacity:0;-webkit-transform:perspective(700px) rotateX(-60deg);transform:perspective(700px) rotateX(-60deg)}.tippy-popper[x-placement^=bottom] [data-animation=fade][data-state=visible]{-webkit-transform:translateY(10px);transform:translateY(10px)}.tippy-popper[x-placement^=bottom] [data-animation=fade][data-state=hidden]{opacity:0;-webkit-transform:translateY(10px);transform:translateY(10px)}.tippy-popper[x-placement^=bottom] [data-animation=shift-away][data-state=visible]{-webkit-transform:translateY(10px);transform:translateY(10px)}.tippy-popper[x-placement^=bottom] [data-animation=shift-away][data-state=hidden]{opacity:0}.tippy-popper[x-placement^=bottom] [data-animation=scale]{-webkit-transform-origin:top;transform-origin:top}.tippy-popper[x-placement^=bottom] [data-animation=scale][data-state=visible]{-webkit-transform:translateY(10px);transform:translateY(10px)}.tippy-popper[x-placement^=bottom] [data-animation=scale][data-state=hidden]{opacity:0;-webkit-transform:translateY(10px) scale(.5);transform:translateY(10px) scale(.5)}.tippy-popper[x-placement^=left] .tippy-backdrop{border-radius:50% 0 0 50%}.tippy-popper[x-placement^=left] .tippy-roundarrow{right:-12px;-webkit-transform-origin:33.33333333% 50%;transform-origin:33.33333333% 50%;margin:3px 0}.tippy-popper[x-placement^=left] .tippy-roundarrow svg{position:absolute;left:0;-webkit-transform:rotate(90deg);transform:rotate(90deg)}.tippy-popper[x-placement^=left] .tippy-arrow{border-left:8px solid #333;border-top:8px solid transparent;border-bottom:8px solid transparent;right:-7px;margin:3px 0;-webkit-transform-origin:0 50%;transform-origin:0 50%}.tippy-popper[x-placement^=left] .tippy-backdrop{-webkit-transform-origin:50% 0;transform-origin:50% 0}.tippy-popper[x-placement^=left] .tippy-backdrop[data-state=visible]{-webkit-transform:scale(1) translate(-50%,-50%);transform:scale(1) translate(-50%,-50%)}.tippy-popper[x-placement^=left] .tippy-backdrop[data-state=hidden]{-webkit-transform:scale(.2) translate(-75%,-50%);transform:scale(.2) translate(-75%,-50%);opacity:0}.tippy-popper[x-placement^=left] [data-animation=shift-toward][data-state=visible]{-webkit-transform:translateX(-10px);transform:translateX(-10px)}.tippy-popper[x-placement^=left] [data-animation=shift-toward][data-state=hidden]{opacity:0;-webkit-transform:translateX(-20px);transform:translateX(-20px)}.tippy-popper[x-placement^=left] [data-animation=perspective]{-webkit-transform-origin:right;transform-origin:right}.tippy-popper[x-placement^=left] [data-animation=perspective][data-state=visible]{-webkit-transform:perspective(700px) translateX(-10px);transform:perspective(700px) translateX(-10px)}.tippy-popper[x-placement^=left] [data-animation=perspective][data-state=hidden]{opacity:0;-webkit-transform:perspective(700px) rotateY(-60deg);transform:perspective(700px) rotateY(-60deg)}.tippy-popper[x-placement^=left] [data-animation=fade][data-state=visible]{-webkit-transform:translateX(-10px);transform:translateX(-10px)}.tippy-popper[x-placement^=left] [data-animation=fade][data-state=hidden]{opacity:0;-webkit-transform:translateX(-10px);transform:translateX(-10px)}.tippy-popper[x-placement^=left] [data-animation=shift-away][data-state=visible]{-webkit-transform:translateX(-10px);transform:translateX(-10px)}.tippy-popper[x-placement^=left] [data-animation=shift-away][data-state=hidden]{opacity:0}.tippy-popper[x-placement^=left] [data-animation=scale]{-webkit-transform-origin:right;transform-origin:right}.tippy-popper[x-placement^=left] [data-animation=scale][data-state=visible]{-webkit-transform:translateX(-10px);transform:translateX(-10px)}.tippy-popper[x-placement^=left] [data-animation=scale][data-state=hidden]{opacity:0;-webkit-transform:translateX(-10px) scale(.5);transform:translateX(-10px) scale(.5)}.tippy-popper[x-placement^=right] .tippy-backdrop{border-radius:0 50% 50% 0}.tippy-popper[x-placement^=right] .tippy-roundarrow{left:-12px;-webkit-transform-origin:66.66666666% 50%;transform-origin:66.66666666% 50%;margin:3px 0}.tippy-popper[x-placement^=right] .tippy-roundarrow svg{position:absolute;left:0;-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.tippy-popper[x-placement^=right] .tippy-arrow{border-right:8px solid #333;border-top:8px solid transparent;border-bottom:8px solid transparent;left:-7px;margin:3px 0;-webkit-transform-origin:100% 50%;transform-origin:100% 50%}.tippy-popper[x-placement^=right] .tippy-backdrop{-webkit-transform-origin:-50% 0;transform-origin:-50% 0}.tippy-popper[x-placement^=right] .tippy-backdrop[data-state=visible]{-webkit-transform:scale(1) translate(-50%,-50%);transform:scale(1) translate(-50%,-50%)}.tippy-popper[x-placement^=right] .tippy-backdrop[data-state=hidden]{-webkit-transform:scale(.2) translate(-25%,-50%);transform:scale(.2) translate(-25%,-50%);opacity:0}.tippy-popper[x-placement^=right] [data-animation=shift-toward][data-state=visible]{-webkit-transform:translateX(10px);transform:translateX(10px)}.tippy-popper[x-placement^=right] [data-animation=shift-toward][data-state=hidden]{opacity:0;-webkit-transform:translateX(20px);transform:translateX(20px)}.tippy-popper[x-placement^=right] [data-animation=perspective]{-webkit-transform-origin:left;transform-origin:left}.tippy-popper[x-placement^=right] [data-animation=perspective][data-state=visible]{-webkit-transform:perspective(700px) translateX(10px);transform:perspective(700px) translateX(10px)}.tippy-popper[x-placement^=right] [data-animation=perspective][data-state=hidden]{opacity:0;-webkit-transform:perspective(700px) rotateY(60deg);transform:perspective(700px) rotateY(60deg)}.tippy-popper[x-placement^=right] [data-animation=fade][data-state=visible]{-webkit-transform:translateX(10px);transform:translateX(10px)}.tippy-popper[x-placement^=right] [data-animation=fade][data-state=hidden]{opacity:0;-webkit-transform:translateX(10px);transform:translateX(10px)}.tippy-popper[x-placement^=right] [data-animation=shift-away][data-state=visible]{-webkit-transform:translateX(10px);transform:translateX(10px)}.tippy-popper[x-placement^=right] [data-animation=shift-away][data-state=hidden]{opacity:0}.tippy-popper[x-placement^=right] [data-animation=scale]{-webkit-transform-origin:left;transform-origin:left}.tippy-popper[x-placement^=right] [data-animation=scale][data-state=visible]{-webkit-transform:translateX(10px);transform:translateX(10px)}.tippy-popper[x-placement^=right] [data-animation=scale][data-state=hidden]{opacity:0;-webkit-transform:translateX(10px) scale(.5);transform:translateX(10px) scale(.5)}.tippy-tooltip{position:relative;color:#fff;border-radius:.25rem;font-size:.875rem;padding:.3125rem .5625rem;line-height:1.4;text-align:center;background-color:#333}.tippy-tooltip[data-size=small]{padding:.1875rem .375rem;font-size:.75rem}.tippy-tooltip[data-size=large]{padding:.375rem .75rem;font-size:1rem}.tippy-tooltip[data-animatefill]{overflow:hidden;background-color:initial}.tippy-tooltip[data-interactive],.tippy-tooltip[data-interactive] .tippy-roundarrow path{pointer-events:auto}.tippy-tooltip[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-tooltip[data-inertia][data-state=hidden]{transition-timing-function:ease}.tippy-arrow,.tippy-roundarrow{position:absolute;width:0;height:0}.tippy-roundarrow{width:18px;height:7px;fill:#333;pointer-events:none}.tippy-backdrop{position:absolute;background-color:#333;border-radius:50%;width:calc(110% + 2rem);left:50%;top:50%;z-index:-1;transition:all cubic-bezier(.46,.1,.52,.98);-webkit-backface-visibility:hidden;backface-visibility:hidden}.tippy-backdrop:after{content:"";float:left;padding-top:100%}.tippy-backdrop+.tippy-content{transition-property:opacity;will-change:opacity}.tippy-backdrop+.tippy-content[data-state=hidden]{opacity:0}</style><link href="../../_assets/623d7d246e8213dbe6aace84_css_fruitpunch-ai.webflow.e858c30b5.css.css" rel="stylesheet" type="text/css"><link href="https://fonts.googleapis.com" rel="preconnect"><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous"><!-- Tracking script removed for offline --><script type="text/javascript" charset="utf-8" src="../../_assets/js_signup-forms_popup_unique-methods_38d3020ee67bdafdc3231e2272e1fa0689108cda_popup.js.js"></script><!-- Tracking script removed for offline --><script type="text/javascript" async="" src="https://s3-us-west-2.amazonaws.com/b2bjsstore/b/4O7Z0HM2DWNX/4O7Z0HM2DWNX.js.gz"></script><script async="" src="../../_assets/mcjs-connected_js_users_274b78179f279b8ff0a6f8164_1de32b2c861555e2768ba71e2.js.js"></script><!-- Tracking script removed for offline --><!-- Tracking script removed for offline --><script src="../../_assets/ajax_libs_webfont_1.6.26_webfont.js.js" type="text/javascript"></script><link rel="stylesheet" href="../../_assets/css.html.css" media="all"><script type="text/javascript">WebFont.load({  google: {    families: ["Atkinson Hyperlegible:regular,700","Roboto Mono:regular,600,700","Inter:100,200,300,regular,500,600,700,800,900"]  }});</script><script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script><link href="../../_assets/623d7d246e8213dbe6aace84_62460f7d0eb74f5a342f656e_32x32.svg.svg" rel="shortcut icon" type="image/x-icon"><link href="../../_assets/623d7d246e8213dbe6aace84_62460f835d0f30b3e1e8e5fb_256x256.svg.svg" rel="apple-touch-icon"><!-- Tracking script removed for offline --><!-- Tracking script removed for offline --><!-- Tracking script removed for offline -->



<!--Tooltip Styling-->
<link rel="stylesheet" href="../../_assets/tippy.js_4_themes_light-border.css.css">
<style>
.blog-post_image-height {
	-webkit-backface-visibility: hidden;
	-moz-backface-visibility: hidden;
	-webkit-transform: translate3d(0, 0, 0);
	-moz-transform: translate3d(0, 0, 0)
}
</style>
<!-- Google tag (gtag.js) -->
<!-- Tracking script removed for offline -->
<!-- Tracking script removed for offline -->

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-PMKPDFGL');</script>
<!-- End Google Tag Manager -->


<script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/274b78179f279b8ff0a6f8164/1de32b2c861555e2768ba71e2.js");</script>

<script>!function () {var reb2b = window.reb2b = window.reb2b || [];if (reb2b.invoked) return;reb2b.invoked = true;reb2b.methods = ["identify", "collect"];reb2b.factory = function (method) {return function () {var args = Array.prototype.slice.call(arguments);args.unshift(method);reb2b.push(args);return reb2b;};};for (var i = 0; i < reb2b.methods.length; i++) {var key = reb2b.methods[i];reb2b[key] = reb2b.factory(key);}reb2b.load = function (key) {var script = document.createElement("script");script.type = "text/javascript";script.async = true;script.src = "https://s3-us-west-2.amazonaws.com/b2bjsstore/b/" + key + "/4O7Z0HM2DWNX.js.gz";var first = document.getElementsByTagName("script")[0];first.parentNode.insertBefore(script, first);};reb2b.SNIPPET_VERSION = "1.0.1";reb2b.load("4O7Z0HM2DWNX");}();</script><script>
const canonicalUrl = "https://app.fruitpunch.ai/article/2024/05/14/the-bear-necessity-of-ai-in-conservation";
if (canonicalUrl != "") {
	var canonicalEl = document.createElement("link");
	canonicalEl.rel = "canonical";
	canonicalEl.href = canonicalUrl;
	document.head.appendChild(canonicalEl);
}
</script><link rel="canonical" href="https://app.fruitpunch.ai/article/2024/05/14/the-bear-necessity-of-ai-in-conservation">

<!-- [Attributes by Finsweet] CMS PrevNext -->
<script async="" src="../../_assets/npm__finsweet_attributes-cmsprevnext_1_cmsprevnext.js.js"></script>

<!-- [Attributes by Finsweet] CMS Nest -->
<script async="" src="../../_assets/npm__finsweet_attributes-cmsnest_1_cmsnest.js.js"></script>
<link rel="stylesheet" type="text/css" href="../../_assets/css_signup-forms_popup_38d3020ee67bdafdc3231e2272e1fa0689108cda_modal.css.css" media="all"><style type="text/css" id=""></style><style type="text/css">#mc_embed_signup input.mce_inline_error { border-color:#6B0505; } #mc_embed_signup div.mce_inline_error { margin: 0 0 1em 0; padding: 5px 10px; background-color:#6B0505; font-weight: bold; z-index: 1; color:#fff; }</style>
<script>
(function() {
  // Intercept fetch calls
  const originalFetch = window.fetch;
  window.fetch = function(...args) {
    const url = args[0];
    
    // Check if this is a Webflow API call
    if (typeof url === 'string' && (url.includes('api.webflow.com') || url.includes('webflow.com/api'))) {
      // Try to load from cache
      const cacheKey = btoa(url).replace(/[^a-zA-Z0-9]/g, '_').substring(0, 100);
      const apiPath = '../_api_data/' + cacheKey + '.json';
      
      return fetch(apiPath)
        .then(response => response.json())
        .then(data => {
          return new Response(data.response.body, {
            status: data.response.status,
            statusText: data.response.statusText,
            headers: data.response.headers
          });
        })
        .catch(() => {
          return originalFetch.apply(this, args);
        });
    }
    
    return originalFetch.apply(this, args);
  };
  
  // Intercept XMLHttpRequest
  const originalXHROpen = XMLHttpRequest.prototype.open;
  const originalXHRSend = XMLHttpRequest.prototype.send;
  
  XMLHttpRequest.prototype.open = function(method, url, ...rest) {
    this._url = url;
    return originalXHROpen.apply(this, [method, url, ...rest]);
  };
  
  XMLHttpRequest.prototype.send = function(...args) {
    if (this._url && (this._url.includes('api.webflow.com') || this._url.includes('webflow.com/api'))) {
      const cacheKey = btoa(this._url).replace(/[^a-zA-Z0-9]/g, '_').substring(0, 100);
      const apiPath = '../_api_data/' + cacheKey + '.json';
      
      fetch(apiPath)
        .then(response => response.json())
        .then(data => {
          Object.defineProperty(this, 'status', { value: data.response.status, writable: false });
          Object.defineProperty(this, 'statusText', { value: data.response.statusText, writable: false });
          Object.defineProperty(this, 'responseText', { value: data.response.body, writable: false });
          Object.defineProperty(this, 'readyState', { value: 4, writable: false });
          
          if (this.onload) this.onload();
          if (this.onreadystatechange) this.onreadystatechange();
        })
        .catch(() => {
          return originalXHRSend.apply(this, args);
        });
      
      return;
    }
    
    return originalXHRSend.apply(this, args);
  };
})();
</script><style>
    .mc-modal, .mc-modal-bg, .mc-banner, #PopupSignupForm_0, [id^="PopupSignupForm"],
    [class*="mc-modal"], [class*="mc-banner"], [class*="popup-overlay"] { 
      display: none !important; 
      visibility: hidden !important;
      opacity: 0 !important;
      pointer-events: none !important;
    }
  </style></head><body style="overflow: visible; zoom: 1;"><div data-w-id="d162ae7a-f1fa-352e-4672-062769f40e40" data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="navbar-2 w-nav"><div class="a-banner-announcement"><div class="a-banner-container-small"><div class="a-banner-text-wrapper"><div class="a-banner-icon"></div></div><div class="a-banner-right-wrapper"><div data-hover="false" data-delay="0" class="w-dropdown"><div class="a-banner-toggle-copy w-dropdown-toggle" id="w-dropdown-toggle-0" aria-controls="w-dropdown-list-0" aria-haspopup="menu" aria-expanded="false" role="button" tabindex="0"><div class="a-caption-copy">For Individuals</div></div><nav class="a-banner-list w-dropdown-list" id="w-dropdown-list-0" aria-labelledby="w-dropdown-toggle-0"></nav></div><div data-hover="false" data-delay="0" class="w-dropdown"><div class="a-banner-toggle w-dropdown-toggle" id="w-dropdown-toggle-1" aria-controls="w-dropdown-list-1" aria-haspopup="menu" aria-expanded="false" role="button" tabindex="0"><a href="../../partners/index.html" class="w-inline-block"><div class="a-caption">For Organizations</div></a></div><nav class="a-banner-list w-dropdown-list" id="w-dropdown-list-1" aria-labelledby="w-dropdown-toggle-1"></nav></div></div></div></div><div class="uui-navbar02_component-3"><div class="uui-navbar02_container-3"><a href="../../index/index.html" class="uui-navbar02_logo-link-3 w-nav-brand" aria-label="home"><div class="uui-logo_component-4"><img src="../../_assets/623d7d246e8213dbe6aace84_624447bdff22b73c4a02373f_Logo-horizontal-colour.svg.svg" loading="lazy" alt="FruitPunch AI logo" class="uui-logo_logotype-3"><img src="../../_assets/623d7d246e8213dbe6aace84_652797dd2601a8abb837b8f7_untitled-ui-logo.png.png" loading="lazy" alt="Logo" class="uui-logo_image-3"></div></a><nav role="navigation" class="uui-navbar02_menu-3 w-nav-menu"><div class="uui-navbar02_menu-left-3"><a href="https://app.fruitpunch.ai/challenges" class="uui-navbar02_link-3 w-nav-link">Challenges</a><div data-hover="true" data-delay="300" data-w-id="d162ae7a-f1fa-352e-4672-062769f40e58" class="uui-navbar02_menu-dropdown-3 w-dropdown"><div class="uui-navbar02_dropdown-toggle-3 w-dropdown-toggle" id="w-dropdown-toggle-2" aria-controls="w-dropdown-list-2" aria-haspopup="menu" aria-expanded="false" role="button" tabindex="0"><div class="uui-dropdown-icon-3 w-embed"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M5 7.5L10 12.5L15 7.5" stroke="currentColor" stroke-width="1.67" stroke-linecap="round" stroke-linejoin="round"></path>
</svg></div><div class="text-block-12">About us </div></div><nav class="dropdown-list w-dropdown-list" id="w-dropdown-list-2" aria-labelledby="w-dropdown-toggle-2"><div class="nav_drop-down-links"><div class="mega-menu_link-items-wrapper"><a href="../../about/index.html" class="mega-menu_link-item" tabindex="0">Our Story</a><a href="../../faq/index.html" class="mega-menu_link-item" tabindex="0">FAQ</a><a href="../../contact/index.html" class="mega-menu_link-item" tabindex="0">Contact us</a></div><div class="dropdown_top-colour is-green"></div></div></nav></div><a href="../../publications/index.html" class="uui-navbar02_link-3 w-nav-link">Blog</a><a href="../../pricing/index.html" class="uui-navbar02_link-3 w-nav-link">Pricing</a></div><div class="uui-navbar02_menu-right-3"><div class="uui-navbar02_button-wrapper-3"><a href="#" class="uui-button-secondary-gray-3 show-tablet w-inline-block"><div>Log in</div></a><a href="https://app.fruitpunch.ai/account/login" class="uui-button-tertiary-gray-3 hide-tablet w-inline-block"><div class="text-block-16">Log in</div></a><a href="https://app.fruitpunch.ai/account/register" class="button outline w-inline-block"><div>Get Started</div></a></div></div></nav><div class="uui-navbar02_menu-button-3 w-nav-button" style="-webkit-user-select: text;" aria-label="menu" role="button" tabindex="0" aria-controls="w-nav-overlay-0" aria-haspopup="menu" aria-expanded="false"><div class="menu-icon_component-3"><div class="menu-icon_line-top-3"></div><div class="menu-icon_line-middle-3"><div class="menu-icon_line-middle-inner-3"></div></div><div class="menu-icon_line-bottom-3"></div></div></div></div></div><a href="https://api.whatsapp.com/message/SO3LKMLLO5GZI1" target="_blank" class="whatsapp w-inline-block"><div class="list-icon-white margin w-embed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M.057 24l1.687-6.163c-1.041-1.804-1.588-3.849-1.587-5.946.003-6.556 5.338-11.891 11.893-11.891 3.181.001 6.167 1.24 8.413 3.488 2.245 2.248 3.481 5.236 3.48 8.414-.003 6.557-5.338 11.892-11.893 11.892-1.99-.001-3.951-.5-5.688-1.448l-6.305 1.654zm6.597-3.807c1.676.995 3.276 1.591 5.392 1.592 5.448 0 9.886-4.434 9.889-9.885.002-5.462-4.415-9.89-9.881-9.892-5.452 0-9.887 4.434-9.889 9.884-.001 2.225.651 3.891 1.746 5.634l-.999 3.648 3.742-.981zm11.387-5.464c-.074-.124-.272-.198-.57-.347-.297-.149-1.758-.868-2.031-.967-.272-.099-.47-.149-.669.149-.198.297-.768.967-.941 1.165-.173.198-.347.223-.644.074-.297-.149-1.255-.462-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.297-.347.446-.521.151-.172.2-.296.3-.495.099-.198.05-.372-.025-.521-.075-.148-.669-1.611-.916-2.206-.242-.579-.487-.501-.669-.51l-.57-.01c-.198 0-.52.074-.792.372s-1.04 1.016-1.04 2.479 1.065 2.876 1.213 3.074c.149.198 2.095 3.2 5.076 4.487.709.306 1.263.489 1.694.626.712.226 1.36.194 1.872.118.571-.085 1.758-.719 2.006-1.413.248-.695.248-1.29.173-1.414z" fill="currentColor"></path></svg></div><h1 class="icontext-copy">Chat with our AI Expert</h1></a><div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div></div><div class="global-elements"><div class="html w-embed"><style>
/* Main Variables */
:root {
  --main-dark: #eb306e;
  --main-light: #fffffe;
}

/* Global Styles */

/* .body {
  font-feature-settings: 'cv08' on, 'salt' on, 'ss02' on, 'cv05' on, 'cv01' on, 'cv06' on, 'cv10' on, 'cv11' on;
}
*/

::selection {
	background: var(--main-dark);
  color: var(--main-light);
  text-shadow: none;
}
img::selection, svg::selection {
	background: transparent;
}

/* Disable / enable clicking on an element and its children  */
.no-click {
	pointer-events: none;
}
.can-click {
	pointer-events: auto;
}

/* Container Max Width 
.container {
  max-width: 1280px;
}
*/
/* Splide settings */
.splide button:disabled {
	opacity: 0.4;
}

.splide .splide__pagination { 
	display: none;
}


/* Image overflow-none and animation fix for Safari */
.blog-post_image-height {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.highlight {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.blog-youtube-video-embed {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

</style></div></div><section class="section"><div class="container is-blog-header"><div class="blog-header"><div class="blog-header_info-wrapper"><div class="blog_published-date">May 14, 2024</div></div><div class="blog-header_content-wrapper"><h1>The Bear Necessity of AI in Conservation</h1><div class="blog-header_intro">The AI for Bears Challenge results which aims to improve the monitoring and identification of bears using advanced computer vision techniques.</div></div><div class="blog-header_image-wrapper"><div class="blog-header_image-height"><img height="100" loading="lazy" alt="" src="../../_assets/62604c2173fafef5f182b55a_6643611db64f259e60a5aaca_header.jpeg.jpeg" class="blog-header_hero-img"></div></div></div></div></section><section class="section"><div class="container is-blog-body"><div class="blog-body_wrapper"><div class="blog-body_content"><div class="blog_rich-text w-richtext"><p>The "AI for Bears" Challenge aims to improve the monitoring and identification of bears using advanced computer vision techniques. Brown bears, critical to ecosystem balance, are notoriously difficult to track due to their vast ranges and lack of unique markings. Traditional monitoring involves invasive methods like physical tagging, which poses significant challenges. Therefore, we focus on developing non-invasive AI technologies to enhance bear conservation efforts.</p>

<p>With our main goal of identifying bears in mind, we formed 4 teams that each tackled 1 part of the Challenge:</p>

<ul>
<li>Team 1: <strong>Bear classification</strong></li>
<li>Team 2: <strong>Bear face detection and Segmentation</strong></li>
<li>Team 3: <strong>Bear Identification</strong></li>
<li>Team 4: <strong>ML pipeline on edge</strong></li>
</ul>

<h2>The challenge in short:</h2>

<p>The main objective of Team 1 was to create efficient AI models that can accurately classify images based on the presence of bears. The goal was to filter and process a large amount of camera trap data so that subsequent AI processes could focus only on relevant bear images. Once accurate classification was established, Team 2 concentrated on precisely detecting and segmenting the bear faces within the images. This was needed to identify individuals, as it isolates the necessary bear features for the subsequent identification process. Team 3 was at the core of the challenge and was tasked with developing AI models that could identify individual bears from the segmented face images. The final team worked on the practical deployment of the AI models developed by Teams 1 to 3. Team 4 focused on optimizing these models to run efficiently on edge devices.</p>

<p>In 10 weeks we build a machine learning pipeline that takes photographs and camera trap imagery, finds a bear face, segments it, and compares it to a database of known bears. Using metric learning, we can find the correct match in more than 90% of the cases!</p>

<p>We leveraged multiple datasets from the BearID Project. The first dataset contained 3300 photographs of bears which we used for training our models. </p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d992a995d46a63e3b7_oIh29TkOkgQdwJWZ-g1GeaLDYXYuTRP3AQYWRw2iN8m2OkcdtUMmplnVw-NDoESjGna1lhInOK1QmNq0hBOKYhusoplFAwBYzmsLAqAC2yvAvAyfoykF_ASyjwtfIMPTvhO9boZGrusa_j_x94dlsSA.jpeg.jpeg" width="100%"></p>

<p>_Figure 1: Example image from the dataset _</p>

<p>The other data existed of ‚Äúchips", the cutout versions of the bear images, and unique bear IDs.</p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d892a995d46a63e364_TrhbzZuxs1S3qwKmK5f5J5iFxeBMw0H3pqLeUUncMxgyhDL5Z6U07ZM4cv-lXGlDy2ZapYokNT6yHYnY3xgnXKV5SDwOuD_U3yr31DVz-O5qje3DlmCTB6MeQMJWS-TRgH7zghSCQoiyP7X4sbKAan0.jpeg.jpeg" alt="An example of a bear chip" title="An example of a bear chip"></p>

<p><em>Figure 2: An example of a bear chip.</em></p>

<h3>Before we dive into the details‚Ä¶</h3>

<p>We would like to express our gratitude for the technical and financial support provided by ARM and NXP which played a crucial role in the success of this Challenge.</p>

<p>ARM contributed to the Challenge by providing their specialized virtual hardware platform and tools tailored for AI development on edge devices. This enabled the teams to optimize their machine learning models for ARM processors. ARM also offered technical expertise, which helped enhance the development and optimization process. Their sponsorship provided essential financial support, which was instrumental in ensuring the Challenge's success.</p>

<p>NXP, on the other hand, contributed to the Challenge by providing advanced microcontrollers (i.MX 93 chip) and processors designed for efficient AI processing in remote settings. They also provided integration support to ensure that this hardware worked seamlessly with the necessary sensors and camera systems used in wildlife monitoring. Alongside ARM, NXP co-sponsored the overall costs of the Challenge, covering technical and operational expenses, which were crucial for the execution of this initiative.</p>

<h2>Classification</h2>

<p>The objective of this team was to develop AI models for the detection of bears in video frames captured from camera traps placed in the wild. Given the edge deployment of these models, size and efficiency were among the main constraints.</p>

<p>Requirements for the solution:</p>

<ul>
<li>Work on camera trap frames, both daytime and nighttime vision</li>
<li>An AI model suitable for edge deployment on low-powered device

<ul>
<li>Small model size</li>
<li>Fast computation</li>
</ul></li>
<li>High recall on identifying bears

<ul>
<li>Missing bears has a high cost</li>
</ul></li>
</ul>

<p>To solve this problem, we chose to train a classification model instead of a detection model because classification models are generally smaller and require less computational resources compared to detection models. However, one drawback of classification models is that they are not designed to learn to localize the subject of an image, meaning they may pick up on features that are useful for minimizing loss but need to be more robust for generalization. Nevertheless, as the final use case doesn't require localization information, we decided to proceed with classification models, prioritizing efficiency.</p>

<p>We chose the MobileNetV3-Small architecture for its efficiency, which is a modern classification CNN specifically designed for this purpose. Moreover, this model is already available in TensorFlow version 2.15, making it easier to use. All the models were initialized using the ‚ÄúImageNet‚Äù weights provided. </p>

<h3>Approach A | Single MobileNetV3-Small</h3>

<p>This approach (Figure 3) consists of a single MobileNetV3-Small trained to classify the images into three classes:</p>

<ul>
<li>empty üå≥</li>
<li>any other animal üê∫</li>
<li>bear üêª</li>
</ul>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d892a995d46a63e367_wep5g0POVpLR0XncHHgANboAWT8zFxLxlteop3qDRZ7iwNxWT0BakdQd619l54ITSsJzPWs18Lnk97y4EWRlJnzqxzfxh2OJrYc5qPidM-eedF7171n0BM7CrPScBJ4Xsz5MyDg1LhHe_k9_BW1Dfrs.png.png" alt="Schematic representation of the Single MobileNet-V3 approach" title="Schematic representation of the Single MobileNet-V3 approach"></p>

<p><em>Figure 3. Schematic representation of the Single MobileNet-V3 approach.</em></p>

<h3>Approach B | Double MobileNetV3-Small Pipeline</h3>

<p>This approach (Figure 4) consists of two MobileNetV3-Small models acting in a sequence. The first model is trained to identify whether the frame is empty or it contains something:</p>

<ul>
<li>empty üå≥</li>
<li>not empty üå≥‚ùå</li>
</ul>

<p>The second model is executed only if the first model detects a non-empty frame (üå≥‚ùå) to identify the presence of either:</p>

<ul>
<li>any other animal üê∫</li>
<li>bear üêª</li>
</ul>

<p>This pipeline was designed to address the problems caused by the recurring backgrounds in the dataset and by the inability of classification models to localize the main subjects of the image. By splitting the pipeline in two, each model can focus on a simpler task at the cost of some computational overhead.</p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d892a995d46a63e36b_kPO2Uqw02Pjsv816ntS0c4CPUh539MfJ6_lrYO-LEUvwcjzvwQ-GF01qLn6ZKs86iSGc7JVbbjFrKwc486YlonY5UvDcF8peAayFvkQgAzhdjWs-Ud4opwxKNw80GQsODjaLvSKCz5h3-FgVzMaa46A.png.png" alt="Schematic representation of the Single MobileNet-V3 approach" title="Schematic for Double MobileNetV3-Small Pipeline approach"></p>

<p><em>Figure 4. Schematic for Double MobileNetV3-Small Pipeline approach.</em></p>

<p>The following metrics were calculated on the same test set mentioned in the data split discussed earlier. These metrics are commonly used in machine learning classification problems: Accuracy, Recall, Precision, and F1 Score. All metrics, except Accuracy, are reported for each individual class in a one-vs-all manner.</p>

<table>
  <tbody><tr>
   <td colspan="2">
   </td>
   <td colspan="3"><strong>Model</strong>
   </td>
  </tr>
  <tr>
   <td colspan="2"><strong>Metric (%)</strong>
   </td>
   <td><strong>Approach A</strong>
<p>
<strong>MobileNetV3-Small</strong>
   </p></td>
   <td><strong>Approach B</strong>
<p>
<strong>Two MobileNetV3-Small Sequential Pipeline</strong>
   </p></td>
   <td><strong>Approach A Quantized</strong>
<p>
<strong>MobileNetV3-Small</strong>
   </p></td>
  </tr>
  <tr>
   <td colspan="2"><strong>Accuracy</strong>
   </td>
   <td>91.70
   </td>
   <td>93.10
   </td>
   <td>82.84
   </td>
  </tr>
  <tr>
   <td rowspan="3"><strong>Recall</strong>
   </td>
   <td>üå≥
   </td>
   <td>80.16
   </td>
   <td>87.30
   </td>
   <td>94.15
   </td>
  </tr>
  <tr>
   <td>üê∫
   </td>
   <td>96.95
   </td>
   <td>97.10
   </td>
   <td>82.91
   </td>
  </tr>
  <tr>
   <td>üêª
   </td>
   <td>93.95
   </td>
   <td>93.23
   </td>
   <td>77.09
   </td>
  </tr>
  <tr>
   <td rowspan="3"><strong>Precision</strong>
   </td>
   <td>üå≥
   </td>
   <td>93.81
   </td>
   <td>91.18
   </td>
   <td>67.96
   </td>
  </tr>
  <tr>
   <td>üê∫
   </td>
   <td>95.08
   </td>
   <td>95.77
   </td>
   <td>97.84
   </td>
  </tr>
  <tr>
   <td>üêª
   </td>
   <td>88.60
   </td>
   <td>92.14
   </td>
   <td>84.57
   </td>
  </tr>
  <tr>
   <td rowspan="3"><strong>F1 Score</strong>
   </td>
   <td>üå≥
   </td>
   <td>86.44
   </td>
   <td>89.20
   </td>
   <td>78.94
   </td>
  </tr>
  <tr>
   <td>üê∫
   </td>
   <td>95.96
   </td>
   <td>96.43
   </td>
   <td>89.76
   </td>
  </tr>
  <tr>
   <td>üêª
   </td>
   <td>91.12
   </td>
   <td>92.68
   </td>
   <td>80.65
   </td>
  </tr>
</tbody></table>

<p><em>Tabel 1. Evaluation metrics for the bear classification models</em></p>

<p>Overall, it can be said that both approaches perform well and to a similar degree. Replicating the empty frames that the models see in each epoch was a fundamental step to increase the performance of the models. </p>

<p>Both approaches have an average accuracy of over 90%, with a drop of ~10% in the quantized version.</p>

<h2>Face Detection &amp; Segmentation</h2>

<p>The goal of the Bear‚Äôs Face Detection and Segmentation team is to provide the image of a bear‚Äôs head with no background given the image of a bear from a camera trap. The image of a bear‚Äôs head with no background is called a ‚Äúchip‚Äù and it‚Äôs used as input for the Bear‚Äôs identification team to re-identify bears. </p>

<p>We combine an object detector and a SAM segmentation model to create a dataset of segmented bears. This dataset is used to fine-tune a YOLO segmentation model. By using this bear‚Äôs segmentation model, we can obtain a bear‚Äôs mask. The combination of a trained bear‚Äôs head detection and a bear segmentation model leads to obtaining a bear‚Äôs head with no background. </p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d992a995d46a63e3b1_uiYEC3us-1q46YmRi2MV-oyLeGpqnF1sVqhBN0OzDeNXwerbVqzzmImbuiwBUhsQCNcdR7gx4DTpS9VBv2b73okZpjO7OdV6e-EYWmWTEpkHid51ZISjdwKC2Rl8-AezMhT5HNRBxxVuGorU01EhM9E.png.png" alt="Training process and inference pipeline of the bear face detection" title="Training process and inference pipeline of the bear face detection">
<em>Figure 5: Training process and inference pipeline of the bear face detection</em></p>

<h3>Training process:</h3>

<p>For the bear‚Äôs face detection, we used a YOLOv8 model. After 20 epochs, this model gives a mAP50 of 0.9, which is almost perfect. This model was trained on the BearID dataset with the corrected bounding boxes from Roboflow. Hyperparameter tuning was not needed. The results of this model are shown in Figure 6.</p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d992a995d46a63e3b4_4ztyivvlrZch86h0T6FfZyW3eV90e-JPtuGgo3FIhxNuMPg2aiD5m0lcBI0XeMDgV_krKM0kKtYMux6qBK9iOcSsLHbqm_CnXxOinGVocI_h-zgVD75nMoKfK67NycmM2Or20goI-QOiQwD81Yg4Qls.png.png" alt="Results of a YOLOv8 bear face detector model" title="Training process and inference pipeline of the bear face detection">
<em>Figure 6: Results of a YOLOv8 bear face detector model.</em></p>

<h3>Bear face segmentation</h3>

<p>We use a YOLOv8n-seg to segment the bear faces. Like the previous model, we obtained a high mAP50 of almost 0.99, after a couple of epochs. Once more, no hyperparameter tuning was needed. The results of the bear face segmentation model are shown in Figure 7.  </p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d992a995d46a63e3bc_nKPRY7SmRJi3UQr3kPzPRg4NXpcWiguzRhuegolgtsdvpCZfYdrD1AjacU3Yd0qOhjDpE8GP1fXvi3JaNDAb7fWy8yK_sHhBKPpVkJZIF_iYaFlfsg0tIc82wuJT5rZr3N1BMehTfbkPbBctREVddF0.png.png" alt="Results of a YOLOv8n-seg bear face segmentation model" title="Results of a YOLOv8n-seg bear face segmentation model"></p>

<p><em>Figure 7: Results of a YOLOv8n-seg bear face segmentation model.</em> </p>

<h2>Bear Identification</h2>

<p>Here we will tackle the main goal of the Challenge: Identifying individual bears. We do this using a technique called Metric learning.</p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d992a995d46a63e3ac_a0Am483FbJTc4wBeFzPTqcRmTR7MKciGoH9V_Xg2Ge01B0zXln-dLSOPhpmFVxDNEoUzWUZ0gZodUDobsmAkPDd_Z5hfGaLGJEJtBkAvkibwV8ZyqsUa4-ARwYaYs8Epd7ERWKEgMslYfebnV_3XNTk.png.png" alt="Complete pipeline to identify bears" title="Complete pipeline to identify bears"></p>

<p><em>Figure 8: Complete pipeline to identify bears.</em></p>

<p>Metric Learning is a process of creating models that produce embedding vectors. These vectors aim to minimize the differences within the embeddings of the same individual and maximize the differences between the embeddings of different individuals. To achieve this, we used various loss functions for metric learning, each with its unique properties and advantages. </p>

<h3>Dataset used</h3>

<p>Due to findings in previous Fruitpunch AI challenges, we only want to use the faces of the bear. Backgrounds can cause the model to use them for identification, i.e. bear X is always photographed in the water, then it will use that feature in the output embeddings. Also, the bodies of bears change over time, which can make it harder for the model to learn useful features. Therefore, the other teams worked on a model that extracts the face and removes the background from images of bears. The output of their model is our input data, and their labels are the bear IDs provided by the bearID experts. During the first part of the challenge, we used a total of 4662 images of 132 bears. Near the end of the challenge, we got access to additional data, increasing the total to 51042 images of 144 bears. </p>

<h3>Feature Extractors and Model Architectures</h3>

<p>Given the constraint of deploying our solution on edge hardware, we prioritized efficiency and performance in selecting our feature extractors. Our exploration included:</p>

<ol>
<li><strong>ResNet-50</strong>: Known for its depth and efficiency, making it a robust choice for feature extraction.</li>
<li><strong>EfficientNet Small</strong>: Optimized for speed and accuracy, particularly well-suited for edge devices.</li>
<li><strong>ConvNext Tiny</strong>: A newer architecture that promises improvements over traditional ConvNets.</li>
</ol>

<p>For each of these architectures, we utilized ImageNet pre-trained models as our starting point. Recognizing the unique requirements of our task, we replaced the models' final classification layers with fully-connected layers of varying output dimensionalities. This modification aims to tailor the networks for our specific objective of bear re-identification.</p>

<h3>Loss Functions</h3>

<p>Our experimentation with loss functions included:</p>

<ol>
<li><strong>Triplet Loss</strong>: Encourages the model to distinguish between anchor, positive, and negative samples.</li>
<li><strong>Circle Loss</strong>: Aims to enhance the discriminative power of the embedding space.</li>
<li><strong>ArcFace Loss</strong>: Focuses on increasing the angular margin between classes to improve separability.</li>
</ol>

<p>These loss functions were chosen for their ability to effectively train models for tasks requiring fine-grained distinction between classes, such as individual animal re-identification.</p>

<h3>Evaluation metrics</h3>

<p>In the realm of bear re-identification, accurately evaluating the performance of our models is paramount. The BearID researchers informed us that they would use our model to generate a list of top candidates for each bear. To match this setting, we use the Hit Rate @ K metric to evaluate performance. This metric is defined as the ratio of predicted bear IDs that contain the correct bear ID. So if we take k = 3, we return a list of three bear ids (with possible duplicates). If it contains the correct one, we have a hit, otherwise we don‚Äôt. The average of this is the hit rate. Specifically, we used 1, 3 and 5 as values for k. </p>

<h3>Results</h3>

<p>The results of the experiments are in the table below. It turns out that the ConvNext_tiny was the best-performing backbone. Also, the larger dataset increased the performance significantly. </p>

<table>
  <tbody><tr>
   <td><strong>Backbone</strong>
   </td>
   <td><strong>Loss function</strong>
   </td>
   <td><strong>Hit Rate @ 1</strong>
   </td>
   <td><strong>Hit Rate @ 3</strong>
   </td>
   <td><strong>Hit Rate @ 5</strong>
   </td>
  </tr>
  <tr>
   <td><strong>ConvNext_tiny</strong>
   </td>
   <td><strong>circle loss</strong>
   </td>
   <td><p style="text-align: right">
<strong>0,938</strong></p>

   </td>
   <td><p style="text-align: right">
<strong>0,963</strong></p>

   </td>
   <td><p style="text-align: right">
<strong>0,972</strong></p>

   </td>
  </tr>
  <tr>
   <td>Efficientnet_v2_s
   </td>
   <td>circle loss
   </td>
   <td><p style="text-align: right">
0,7867</p>

   </td>
   <td><p style="text-align: right">
0,889</p>

   </td>
   <td><p style="text-align: right">
0,893 </p>

   </td>
  </tr>
  <tr>
   <td>ResNet-50
   </td>
   <td>circle loss
   </td>
   <td><p style="text-align: right">
0,678</p>

   </td>
   <td><p style="text-align: right">
0,819</p>

   </td>
   <td><p style="text-align: right">
0,862</p>

   </td>
  </tr>
  <tr>
   <td colspan="5"><em>Table 2: Hit rate @ k scores for different backbones</em>
   </td>
  </tr>
</tbody></table>

<p><br></p>

<p>Next to training and fine-tuning our network, we also fine-tuned the open-source MegaDescriptor model. This is a transformer-based architecture specifically trained on animal re-identification tasks. Different sizes of the model exist. We used the smallest model, ie. tiny, to align with our objective to run the model on edge hardware.</p>

<p>As shown in Table 3, this approach already achieves remarkable performance. </p>

<table>
  <tbody><tr>
   <td><strong>Hit Rate @ 1</strong>
   </td>
   <td><strong>Hit Rate @ 3</strong>
   </td>
   <td><strong>Hit Rate @ 5</strong>
   </td>
  </tr>
  <tr>
   <td>0,905
   </td>
   <td>0,943
   </td>
   <td>0,956
   </td>
  </tr>
  <tr>
   <td colspan="3"><em>Table 3: hit rate @ k scores for fine-tuned MegaDescriptor tiny</em>
   </td>
  </tr>
</tbody></table>

<p><br></p>

<h2>ML Pipeline on Edge</h2>

<p>The Edge Pipeline team was tasked with optimizing the performance of the Pytorch models on the NXP i.MX 93 chip. We selected the i.MX 93 chip because it has an NPU that accelerates machine-learning model operations. The team worked on converting the models to the appropriate frameworks that enable acceleration on the chip. Additionally, they quantized the models to ensure efficient inference performance.</p>

<h3>The i.MX 93 chip</h3>

<p>The i.MX 93 chip is placed on an evaluation board that has 2 GB of working memory and 16 GB of storage. The evaluation kit comes with a camera interface that allows for connection to bear trap cameras. What's more, the chip is equipped with an Arm Ethos U-65 MicroNPU. This integration enables it to hasten the different models' operations. The NPU speeds up critical machine learning operations such as matrix multiplications and convolutions, which are necessary for quick processing of such tasks.</p>

<p>The bear identification pipeline comprises multiple components and uses different models, so several models needed to be converted. Figure 9 illustrates the pipeline and its various components. The numbers in red indicate the different components of the pipeline where a machine learning model was utilized. These components were crucial for our team, as they can be accelerated.</p>

<table>
  <tbody><tr>
   <td>Pipeline part
   </td>
   <td>Objective
   </td>
   <td>Model
   </td>
  </tr>
  <tr>
   <td>1 
   </td>
   <td>detecting bears in the frame
   </td>
   <td>MobileNetV3 
   </td>
  </tr>
  <tr>
   <td>2
   </td>
   <td>detecting and segmenting the bear faces
   </td>
   <td>Yolov8
   </td>
  </tr>
  <tr>
   <td>3
   </td>
   <td>matching bear faces
   </td>
   <td>ConvNextTiny
   </td>
  </tr>
</tbody></table>

<p><em>The most crucial part of the pipeline is the model for component 1, as it is the part that runs the most frequently. The remaining components of the pipeline will only run when a bear is actually detected in a frame. Given that component 1 runs the most, it is important to make this component as efficient as possible to reduce energy consumption. Therefore, having a quantized model that can be accelerated is of great importance.</em></p>

<p><img src="../../_assets/62604c2173fafef5f182b55a_664358d992a995d46a63e3a9_Kw2mEO0eefGySam1GybWQyzJ3Kszd9UM0UzRNFI53bb8OSgOD55rVlrak27XULVm6PvHgpZe-_zJAaJSFMyPKhPb1fXA3cuVaVVJgxwGNcC2MbUsIxxKZzJRAUCSQ0XFiheC3zjVbsR8N--krSGKJE0.png.png" alt="Components of the Bear identification pipeline" title="Components of the Bear identification pipeline"></p>

<p><em>Figure 9: Components of the Bear identification pipeline</em></p>

<h3>Results</h3>

<p>After converting the classification, detection, and segmentation models, we compared the inference speed and model performance of the original and converted models. The results are summarized in Table 4. For the classification model, we observed a significant increase in inference speed, with the time decreasing from 38 ms to 2 ms. However, there was a notable decrease in performance in terms of recall, dropping from 93.4 to 77.1. As for the detection and segmentation model, we also noticed a substantial boost in inference speed, with the converted model showing a 5-fold increase, decreasing from 1500 ms to 300 ms. The gains in inference speed also resulted in a significant reduction in power consumption.</p>

<table>
  <tbody><tr>
   <td><strong>Pipeline Component</strong>
   </td>
   <td><strong>Model</strong>
   </td>
   <td><strong>iMX 93</strong>
   </td>
   <td><strong>Inference Time</strong>
   </td>
   <td><strong>Recall</strong>
<p>
<strong>before</strong>
</p><p>
<strong>conversion</strong>
   </p></td>
   <td><strong>Recall</strong>
<p>
<strong>after</strong>
</p><p>
<strong>conversion</strong>
   </p></td>
  </tr>
  <tr>
   <td>1. Classification
   </td>
   <td>MobilenetV3
   </td>
   <td>CPU
   </td>
   <td>~38 ms
   </td>
   <td>~ 94
   </td>
   <td>93.4 (unquantized)
   </td>
  </tr>
  <tr>
   <td>1. Classification
   </td>
   <td>MobilenetV3
   </td>
   <td><strong>NPU</strong>
   </td>
   <td><strong>~2 ms</strong>
   </td>
   <td>~ 94
   </td>
   <td><strong>77.1 (quantized)</strong>
   </td>
  </tr>
  <tr>
   <td>2. Detection
<p>
and Segmentation
   </p></td>
   <td>Yolov8
   </td>
   <td>CPU
   </td>
   <td>~1500 ms
   </td>
   <td>
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>2. Detection
<p>
and Segmentation
   </p></td>
   <td>Yolov8
   </td>
   <td><strong>NPU</strong>
   </td>
   <td><strong>~300 ms</strong>
   </td>
   <td>
   </td>
   <td>
   </td>
  </tr>
</tbody></table>

<p>_Table 4: Results when performing inference on the converted models _</p>

<h2>Concluding</h2>

<p>As we wrap up the "AI for Bears" Challenge, we celebrate significant advancements in using AI to identify bears non-invasively, setting a new standard in wildlife conservation. Over ten weeks, our dedicated teams developed a sophisticated machine-learning pipeline that efficiently identifies individual bears using non-invasive AI technologies!</p>

<p>A heartfelt thanks to our partners, ARM and NXP, for their crucial support, and to all participants whose expertise and dedication have driven this project's success!</p>

<p>Looking ahead, we're excited to see the implementation of these models in the wild.</p>

<p>Big thank you to everyone involved for making this challenge a success: </p>

<p>Ed Miller, Melanie Clapham, Mary Bennion, Brian de Bart, Martijn van der Linden, Laurens Polgar, Hiram Rayo Torres Rodriquez, Matthias Wilkens, David Tischler, Adam, Bart Emons, Carmen Martinez Barbosa, Arthur Caillua, Matthieu Fraissinet-Tachet, Tristan Koskamp, Meredith Palmer, Anton Alvarez, Thor Veen, Christos Panagiotopoulos, Simon Dahrs, Yuri Shlyakhter, Thea Shin, Davide Coppola, John Cooper, Gaspard Bos, Jesse Wiers, Paul L, Jaka Cikaƒç, Prishani Sokay, Sako Arts, Dorian Groen</p>
</div><div class="w-dyn-list"><div role="list" class="collection-list w-dyn-items"><div role="listitem" class="collection-item w-dyn-item"><div class="tag">AI for Good</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">AI for Wildlife</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Artificial Intelligence</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Computer vision</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Deep Learning</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Edge Computing</div></div><div role="listitem" class="collection-item w-dyn-item"><div class="tag">Challenge results</div></div></div></div></div><div class="blog-body_social"><div class="blog_right-col-sticky-wrapper"><div style="padding-top:0%" class="video-2 w-dyn-bind-empty w-video w-embed"></div><div class="blog-body_social-wrapper"><div class="blog-body_social-content"><div class="blog-social_icon-wrapper"><div class="blog_social-heading">Subscribe to our newsletter</div><div class="icon-embed _32x32 w-embed"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg">
<circle cx="16" cy="16" r="16" fill="#FDEAF1"></circle>
<path d="M21 25.25H11C10.59 25.25 10.25 24.91 10.25 24.5C10.25 24.09 10.59 23.75 11 23.75H21C23.86 23.75 25.25 22.36 25.25 19.5V12.5C25.25 9.64 23.86 8.25 21 8.25H11C8.14 8.25 6.75 9.64 6.75 12.5C6.75 12.91 6.41 13.25 6 13.25C5.59 13.25 5.25 12.91 5.25 12.5C5.25 8.85 7.35 6.75 11 6.75H21C24.65 6.75 26.75 8.85 26.75 12.5V19.5C26.75 23.15 24.65 25.25 21 25.25Z" fill="#EB306E"></path>
<path d="M15.9998 16.87C15.1598 16.87 14.3098 16.61 13.6598 16.08L10.5298 13.58C10.2098 13.32 10.1498 12.85 10.4098 12.53C10.6698 12.21 11.1398 12.15 11.4598 12.41L14.5898 14.91C15.3498 15.52 16.6398 15.52 17.3998 14.91L20.5298 12.41C20.8498 12.15 21.3298 12.2 21.5798 12.53C21.8398 12.85 21.7898 13.33 21.4598 13.58L18.3298 16.08C17.6898 16.61 16.8398 16.87 15.9998 16.87Z" fill="#EB306E"></path>
<path d="M12 21.25H6C5.59 21.25 5.25 20.91 5.25 20.5C5.25 20.09 5.59 19.75 6 19.75H12C12.41 19.75 12.75 20.09 12.75 20.5C12.75 20.91 12.41 21.25 12 21.25Z" fill="#EB306E"></path>
<path d="M9 17.25H6C5.59 17.25 5.25 16.91 5.25 16.5C5.25 16.09 5.59 15.75 6 15.75H9C9.41 15.75 9.75 16.09 9.75 16.5C9.75 16.91 9.41 17.25 9 17.25Z" fill="#EB306E"></path>
</svg></div><p class="paragraph">Be the first to know when a new AI for Good challenge is launched. Keep up do date with the latest AI&nbsp;for Good news. </p></div></div><div class="blog-body_newsletter-wrapper"><div id="newsletter-form" data-w-id="f0d3febe-68b2-c3a3-e159-9a6fdebd0168" class="blog_newsletter-form-wrapper w-form"><form id="wf-form-EMAIL" name="wf-form-EMAIL" data-name="EMAIL" action="https://fruitpunch.us20.list-manage.com/subscribe/post?u=274b78179f279b8ff0a6f8164&amp;amp;id=b25b3d594e" method="post" class="blog_newsletter-form" data-wf-page-id="661d09a4061bc8f0cc423c89" data-wf-element-id="f0d3febe-68b2-c3a3-e159-9a6fdebd0169" aria-label="EMAIL"><div class="blog_newsletter-input-wrapper"><div class="w-embed w-script"><div id="mc_embed_shell">
      <link href="" rel="stylesheet" type="text/css">
  <style type="text/css">
        #mc_embed_signup{background:#f7f8f8; false;clear:left; font:1em; font-weight:600; Inter, sans-serif; width: 100%;}
        /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
           We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
    
        <div id="mc_embed_signup_scroll">
            <div class="indicates-required" style="color:#000000;float:right;font-weight:300"><span class="asterisk">*</span> indicates required</div>
            <div class="mc-field-group"><label for="mce-EMAIL" style="color:#000000;margin-bottom:5px;display:block">Email Address <span class="asterisk">*</span></label><input type="email" name="EMAIL" class="input-field w-input required email" id="mce-EMAIL" required="" value=""></div>
        <div id="mce-responses" class="clear" style="color:#000000;font-weight:300;font-style:italic">
            <div class="response" id="mce-error-response" style="display: none;"></div>
            <div class="response" id="mce-success-response" style="display: none;"></div>
        </div><div aria-hidden="true" style="position: absolute; left: -5000px;"><input type="text" name="b_274b78179f279b8ff0a6f8164_b25b3d594e" tabindex="-1" value=""></div><div class="clear"><input type="submit" name="subscribe" id="mc-embedded-subscribe" style="margin-top:5px" class="button outline" value="Subscribe"></div>
    </div>

</div>
<!-- Tracking script removed for offline --><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='MMERGE1';ftypes[1]='phone';fnames[2]='MMERGE2';ftypes[2]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></div></div></div></form><div class="success-message w-form-done" tabindex="-1" role="region" aria-label="EMAIL success"><div class="success-message_heading">Thank you!</div><p class="success-message_sub-heading">We‚Äôve just sent you a confirmation email.</p><p class="success-message_p">We know, this can be annoying, but we want to make sure we don‚Äôt spam anyone. Please, check out your inbox and confirm the link in the email.<br><br>Once confirmed, you‚Äôll be ready to go!</p></div><div class="error-message w-form-fail" tabindex="-1" role="region" aria-label="EMAIL failure"><div class="error-message-p">Oops! Something went wrong while submitting the form.</div></div></div></div></div><div class="blog_next-item-wrapper"></div></div></div></div></div></section><script src="../../_assets/js_signup-forms_popup_unique-methods_embed.js.js" type="text/javascript"></script><section class="section related"><div class="container is-sm-padding is-btm related"><h3>You may also like</h3><div class="w-dyn-list"><div role="list" class="w-dyn-items w-row"><div role="listitem" class="w-dyn-item w-col w-col-6"><a data-w-id="273a5006-de60-6f4f-9030-607401257248" href="../prioritizing-essential-care-with-ai/index.html" class="blog_post-card w-inline-block"><div class="blog-post_image-wrapper"><div class="blog-post_image-height"><img height="100" loading="lazy" style="transform: translate3d(0px, 0px, 0px) scale3d(1, 1, 1) rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg); transform-style: preserve-3d;" src="../../_assets/62604c2173fafef5f182b55a_6798efc44097af4454e6f6f8_header.jpeg.jpeg" alt="" class="blog-post_img" data-clarity-loaded="2cy97u"></div></div><div class="blog-post_content-wrapper"><div class="blog-post_date">January 28, 2025</div><h3>Prioritizing Essential Care with AI</h3><p class="blog-page_post-p">Advancing Neonatal Care: The Role of IMPALA and AI in Improving Early Diagnosis and Treatment</p><address fs-cmsnest-collection="categories"></address></div></a></div><div role="listitem" class="w-dyn-item w-col w-col-6"><a data-w-id="273a5006-de60-6f4f-9030-607401257248" href="index.html" aria-current="page" class="blog_post-card w-inline-block w--current"><div class="blog-post_image-wrapper"><div class="blog-post_image-height"><img height="100" loading="lazy" style="transform: translate3d(0px, 0px, 0px) scale3d(1, 1, 1) rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg); transform-style: preserve-3d;" src="../../_assets/62604c2173fafef5f182b55a_6643611db64f259e60a5aaca_header.jpeg.jpeg" alt="" class="blog-post_img"></div></div><div class="blog-post_content-wrapper"><div class="blog-post_date">May 14, 2024</div><h3>The Bear Necessity of AI in Conservation</h3><p class="blog-page_post-p">The AI for Bears Challenge results which aims to improve the monitoring and identification of bears using advanced computer vision techniques.</p><address fs-cmsnest-collection="categories"></address></div></a></div></div></div></div></section><div class="global-elements"><div class="html w-embed"><style>
/* Main Variables */
:root {
  --main-dark: #eb306e;
  --main-light: #fffffe;
}

/* Global Styles */

/* .body {
  font-feature-settings: 'cv08' on, 'salt' on, 'ss02' on, 'cv05' on, 'cv01' on, 'cv06' on, 'cv10' on, 'cv11' on;
}
*/

::selection {
	background: var(--main-dark);
  color: var(--main-light);
  text-shadow: none;
}
img::selection, svg::selection {
	background: transparent;
}

/* Disable / enable clicking on an element and its children  */
.no-click {
	pointer-events: none;
}
.can-click {
	pointer-events: auto;
}

/* Container Max Width 
.container {
  max-width: 1280px;
}
*/
/* Splide settings */
.splide button:disabled {
	opacity: 0.4;
}

.splide .splide__pagination { 
	display: none;
}


/* Image overflow-none and animation fix for Safari */
.blog-post_image-height {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.highlight {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

.blog-youtube-video-embed {
-webkit-backface-visibility: hidden;
-moz-backface-visibility: hidden;
-webkit-transform: translate3d(0, 0, 0);
-moz-transform: translate3d(0, 0, 0)
}

</style></div></div><footer class="uui-footer03_component-2"><div class="uui-page-padding-5"><div class="uui-container-large-7"><div class="uui-padding-vertical-xlarge-2"><div class="w-layout-grid uui-footer03_top-wrapper-2"><div class="uui-footer03_left-wrapper-2"><a href="#" class="uui-footer03_logo-link-2 w-nav-brand"><div class="uui-logo_component-5"><img src="../../_assets/623d7d246e8213dbe6aace84_624fffdc005e2601af3e69d2_Fp-logo-white.svg.svg" loading="lazy" alt="Untitled UI logotext" class="uui-logo_logotype-4" data-clarity-loaded="1mc3byk"><img src="../../_assets/623d7d246e8213dbe6aace84_652797dd2601a8abb837b8f7_untitled-ui-logo.png.png" loading="lazy" alt="Logo" class="uui-logo_image-4"></div></a><div class="uui-footer03_details-wrapper-2"><div class="uui-text-size-medium-4">Train your AI skills and achieve your learning goals by solving real-world Challenges mentored by experts.</div></div><a href="https://app.fruitpunch.ai/account/register" class="button w-button">Start making impact</a><div class="w-layout-grid uui-footer03_social-list-2"><a href="https://www.facebook.com/FruitPunchAI/" target="_blank" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M24 12C24 5.37258 18.6274 0 12 0C5.37258 0 0 5.37258 0 12C0 17.9895 4.3882 22.954 10.125 23.8542V15.4688H7.07812V12H10.125V9.35625C10.125 6.34875 11.9166 4.6875 14.6576 4.6875C15.9701 4.6875 17.3438 4.92188 17.3438 4.92188V7.875H15.8306C14.34 7.875 13.875 8.80008 13.875 9.75V12H17.2031L16.6711 15.4688H13.875V23.8542C19.6118 22.954 24 17.9895 24 12Z" fill="currentColor"></path>
</svg></div></a><a href="https://www.instagram.com/fruitpunchai/" target="_blank" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 2.16094C15.2063 2.16094 15.5859 2.175 16.8469 2.23125C18.0188 2.28281 18.6516 2.47969 19.0734 2.64375C19.6313 2.85938 20.0344 3.12188 20.4516 3.53906C20.8734 3.96094 21.1313 4.35938 21.3469 4.91719C21.5109 5.33906 21.7078 5.97656 21.7594 7.14375C21.8156 8.40937 21.8297 8.78906 21.8297 11.9906C21.8297 15.1969 21.8156 15.5766 21.7594 16.8375C21.7078 18.0094 21.5109 18.6422 21.3469 19.0641C21.1313 19.6219 20.8688 20.025 20.4516 20.4422C20.0297 20.8641 19.6313 21.1219 19.0734 21.3375C18.6516 21.5016 18.0141 21.6984 16.8469 21.75C15.5813 21.8063 15.2016 21.8203 12 21.8203C8.79375 21.8203 8.41406 21.8063 7.15313 21.75C5.98125 21.6984 5.34844 21.5016 4.92656 21.3375C4.36875 21.1219 3.96563 20.8594 3.54844 20.4422C3.12656 20.0203 2.86875 19.6219 2.65313 19.0641C2.48906 18.6422 2.29219 18.0047 2.24063 16.8375C2.18438 15.5719 2.17031 15.1922 2.17031 11.9906C2.17031 8.78438 2.18438 8.40469 2.24063 7.14375C2.29219 5.97187 2.48906 5.33906 2.65313 4.91719C2.86875 4.35938 3.13125 3.95625 3.54844 3.53906C3.97031 3.11719 4.36875 2.85938 4.92656 2.64375C5.34844 2.47969 5.98594 2.28281 7.15313 2.23125C8.41406 2.175 8.79375 2.16094 12 2.16094ZM12 0C8.74219 0 8.33438 0.0140625 7.05469 0.0703125C5.77969 0.126563 4.90313 0.332812 4.14375 0.628125C3.35156 0.9375 2.68125 1.34531 2.01563 2.01562C1.34531 2.68125 0.9375 3.35156 0.628125 4.13906C0.332812 4.90313 0.126563 5.775 0.0703125 7.05C0.0140625 8.33437 0 8.74219 0 12C0 15.2578 0.0140625 15.6656 0.0703125 16.9453C0.126563 18.2203 0.332812 19.0969 0.628125 19.8563C0.9375 20.6484 1.34531 21.3188 2.01563 21.9844C2.68125 22.65 3.35156 23.0625 4.13906 23.3672C4.90313 23.6625 5.775 23.8687 7.05 23.925C8.32969 23.9812 8.7375 23.9953 11.9953 23.9953C15.2531 23.9953 15.6609 23.9812 16.9406 23.925C18.2156 23.8687 19.0922 23.6625 19.8516 23.3672C20.6391 23.0625 21.3094 22.65 21.975 21.9844C22.6406 21.3188 23.0531 20.6484 23.3578 19.8609C23.6531 19.0969 23.8594 18.225 23.9156 16.95C23.9719 15.6703 23.9859 15.2625 23.9859 12.0047C23.9859 8.74688 23.9719 8.33906 23.9156 7.05938C23.8594 5.78438 23.6531 4.90781 23.3578 4.14844C23.0625 3.35156 22.6547 2.68125 21.9844 2.01562C21.3188 1.35 20.6484 0.9375 19.8609 0.632812C19.0969 0.3375 18.225 0.13125 16.95 0.075C15.6656 0.0140625 15.2578 0 12 0Z" fill="currentColor"></path>
<path d="M12 5.83594C8.59688 5.83594 5.83594 8.59688 5.83594 12C5.83594 15.4031 8.59688 18.1641 12 18.1641C15.4031 18.1641 18.1641 15.4031 18.1641 12C18.1641 8.59688 15.4031 5.83594 12 5.83594ZM12 15.9984C9.79219 15.9984 8.00156 14.2078 8.00156 12C8.00156 9.79219 9.79219 8.00156 12 8.00156C14.2078 8.00156 15.9984 9.79219 15.9984 12C15.9984 14.2078 14.2078 15.9984 12 15.9984Z" fill="currentColor"></path>
<path d="M19.8469 5.59214C19.8469 6.38902 19.2 7.0312 18.4078 7.0312C17.6109 7.0312 16.9688 6.38433 16.9688 5.59214C16.9688 4.79526 17.6156 4.15308 18.4078 4.15308C19.2 4.15308 19.8469 4.79995 19.8469 5.59214Z" fill="currentColor"></path>
</svg></div></a><a href="https://twitter.com/fruitpunchai?s=20&amp;t=r8C8DvQEjIrOl24_F6r5FQ" target="_blank" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><!--?xml version="1.0" encoding="UTF-8"?-->
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" aria-hidden="true" class="r-13v1u17 r-4qtqp9 r-yyyyoo r-16y2uox r-8kz0gk r-dnmrzs r-bnwqim r-1plcrui r-lrvibr r-lrsllp"><g><path d="M14.258 10.152L23.176 0h-2.113l-7.747 8.813L7.133 0H0l9.352 13.328L0 23.973h2.113l8.176-9.309 6.531 9.309h7.133zm-2.895 3.293l-.949-1.328L2.875 1.56h3.246l6.086 8.523.945 1.328 7.91 11.078h-3.246zm0 0" fill="CurrentColor"></path></g></svg></div></a><a href="https://www.linkedin.com/company/fruitpunchai/" class="uui-footer03_social-link-2 w-inline-block"><div class="social-icon-3 w-embed"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M22.2234 0H1.77187C0.792187 0 0 0.773438 0 1.72969V22.2656C0 23.2219 0.792187 24 1.77187 24H22.2234C23.2031 24 24 23.2219 24 22.2703V1.72969C24 0.773438 23.2031 0 22.2234 0ZM7.12031 20.4516H3.55781V8.99531H7.12031V20.4516ZM5.33906 7.43438C4.19531 7.43438 3.27188 6.51094 3.27188 5.37187C3.27188 4.23281 4.19531 3.30937 5.33906 3.30937C6.47813 3.30937 7.40156 4.23281 7.40156 5.37187C7.40156 6.50625 6.47813 7.43438 5.33906 7.43438ZM20.4516 20.4516H16.8937V14.8828C16.8937 13.5562 16.8703 11.8453 15.0422 11.8453C13.1906 11.8453 12.9094 13.2937 12.9094 14.7891V20.4516H9.35625V8.99531H12.7687V10.5609H12.8156C13.2891 9.66094 14.4516 8.70938 16.1813 8.70938C19.7859 8.70938 20.4516 11.0813 20.4516 14.1656V20.4516Z" fill="currentColor"></path>
</svg></div></a></div></div><div id="w-node-_3c7e8777-b0d9-0932-e324-81bb1cd3af93-1cd3af7b" class="w-layout-grid uui-footer03_menu-wrapper-2"><div class="uui-footer03_link-list-2"><a href="https://app.fruitpunch.ai/challenges" class="uui-footer03_link-2 w-inline-block"><div class="text-block-13">Challenges</div></a><a href="../../about/index.html" class="uui-footer03_link-2 w-inline-block"><div class="text-block-14">About us</div></a><a href="../../faq/index.html" class="uui-footer03_link-2 text-block-14 w-inline-block"><div>FAQ</div><div class="uui-badge-small-success-4"><div>New</div></div></a><a href="../../contact/index.html" class="uui-footer03_link-2 w-inline-block"><div>Contact us</div></a><a href="../../publications/index.html" class="uui-footer03_link-2 w-inline-block"><div>Blog</div></a><a href="../../pricing/index.html" class="uui-footer03_link-2 w-inline-block"><div>Pricing</div></a></div><div class="uui-footer03_link-list-2"><a href="#" class="uui-footer03_link-2-copy w-inline-block"><div>Our Labs</div></a><a href="../../labs/ai-for-wildlife-lab/index.html" class="uui-footer03_link-2 w-inline-block"><div>AI for Wildlife Lab</div></a><a href="../../labs/ai-for-earth-lab/index.html" class="uui-footer03_link-2 w-inline-block"><div>AI for Earth Lab</div></a><a href="../../labs/ai-for-health-lab/index.html" class="uui-footer03_link-2 w-inline-block"><div>AI for Health Lab</div></a><a href="#" class="uui-footer03_link-2 w-inline-block"></a><a href="#" class="uui-footer03_link-2 w-inline-block"></a><a href="#" class="uui-footer03_link-2 w-inline-block"></a></div></div></div><div class="uui-footer03_bottom-wrapper-2"><div class="uui-text-size-small-4 text-color-gray500">Copyright ¬© 2023 FruitPunch AI B.V. All rights reserved.</div><div class="w-layout-grid uui-footer03_legal-list-2"><a href="../../terms-of-use/index.html" class="uui-footer03_legal-link-2">Terms of Use</a><a href="../../legal-privacy/index.html" class="uui-footer03_legal-link-2">Privacy Policy</a><a href="#" class="uui-footer03_legal-link-2">Cookies</a></div></div></div></div></div></footer><script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=623d7d246e8213dbe6aace84" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script><script src="../../_assets/623d7d246e8213dbe6aace84_js_webflow.schunk.b7cad701f94860c2.js.js" type="text/javascript"></script><script src="../../_assets/623d7d246e8213dbe6aace84_js_webflow.schunk.bb3b2bf26788d119.js.js" type="text/javascript"></script><script src="../../_assets/623d7d246e8213dbe6aace84_js_webflow.9d5bcaf8.33261154d2049663.js.js" type="text/javascript"></script><!--Tooltip Scripts & Settings-->
<script src="../../_assets/popper.js_1.js@1"></script>
<script src="../../_assets/tippy.js_4.js@4"></script>
<script>
tippy('.tooltip', {        
 animation: 'fade',    
 duration: 200,      
 arrow: true,          
 delay: [0, 50],      
 arrowType: 'sharp',  
 theme: 'light-border',        
 maxWidth: 220,    
 interactive: true,
})
</script>
<!-- Tracking script removed for offline --><script type="text/javascript">
(function(l) {
if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
window.lintrk.q=[]}
var s = document.getElementsByTagName("script")[0];
var b = document.createElement("script");
b.type = "text/javascript";b.async = true;
b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
s.parentNode.insertBefore(b, s);})(window.lintrk);
</script>
<!-- LinkedIn tracking pixel removed --></body></html>